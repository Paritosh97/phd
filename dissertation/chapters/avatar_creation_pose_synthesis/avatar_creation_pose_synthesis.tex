\documentclass[../../main.tex]{subfiles}
\begin{document}
\chapter{Avatar Creation and Pose Synthesis}
\label{ch:avatar_creation_pose_synthesis}

Before we can animate \gls{sl}, a well-crafted avatar is essential. This chapter explores the avatar creation process and integrates rigging layers to the pre-existing low-level AZee synthesizor. Rigging, which sets up a skeletal structure to drive a character's mesh, is critical for achieving realistic movement and deformation, particularly in capturing the nuanced expressions of \gls{sl}.

The layered approach provides a sophisticated method for managing these complexities. By dividing the rigging process into distinct layers—each addressing specific aspects of movement and deformation—we gain greater control and flexibility. This strategy enables the synthesis system to function with multiple avatars and is faster and more accurate than the previous synthesizor.

In this chapter, section~\ref{ch:avatar_creation_pose_synthesis:proc_rig_signing_avatars} introduces a procedural rigging system for signing avatars, which automates the rigging process and enables the generation of complex \gls{sl} gestures. Section~\ref{ch:avatar_creation_pose_synthesis:results} presents the results of the implementation, and section~\ref{ch:avatar_creation_pose_synthesis:evaluation} evaluates the performance, accuracy, and quality of the animation. Finally, section~\ref{ch:avatar_creation_pose_synthesis:conclusion} concludes the chapter and discusses future work.

\section{Procedural Rigging for Signing Avatars}
\label{ch:avatar_creation_pose_synthesis:proc_rig_signing_avatars}

Rigs have been used in computer animation for decades to control the movement and deformation of characters. Traditional rigging systems typically consist of a hierarchical structure of bones, controllers, and constraints that define how the character’s mesh deforms in response to movement.

A signing avatar differs significantly from the traditional systems. To begin with, no \gls{sl} uses the lower body in its articulation. This means that the rigging system for a signing avatar can be simplified to focus on the upper body as shown in figure~\ref{ref:upper_body_avatar}. 

\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\textwidth]{chapters/rigging_layers/images/upper_body_avatar.png}
    \caption{An example of an upper body avatar rig}
    \label{ref:upper_body_avatar}
\end{figure}

However for procedurally animating a signing avatar, the complexity of \gls{sl} gestures and expressions poses unique challenges and necessitates a more sophisticated rigging system.

\subsection{AZee Blender interface}
\label{ch:avatar_creation_pose_synthesis:proc_rig_signing_avatars:azee_blender_interface}

The previous low level synthesizor for AZee~\cite{nunnari2018animating} was based on an armature in blender. These bones and sites were mapped to a \emph{SkelSpec} structure. Similarly, AZee's abstract posture was mapped to the \emph{SkelSpec} structure as well. The \emph{SkelSpec} structure thus formed as an intermediate bridge between an Animated AZee Score and the armature (figure~\ref{fig:old_interface}). 

\begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{chapters/rigging_layers/images/old_interface.png}
    \caption{The old interface of the low level synthesizor for AZee}
    \label{fig:old_interface}
\end{figure}

Due to the complexity of the \emph{SkelSpec} structure, the animation had too much latency. One of the first contributions of our work was to create a new interface for the low level synthesizor for AZee. This new interface didn't have a \emph{SkelSpec} (figure~\ref{fig:new_interface}) and interacted directly with the blender armature. 

\begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{chapters/rigging_layers/images/new_interface.png}
    \caption{The new interface of the low level synthesizor for AZee}
    \label{fig:new_interface}
\end{figure}

\subsection{Sites on the Avatar}
\label{ch:avatar_creation_pose_synthesis:proc_rig_signing_avatars:auto_site_generation}

The AZee low-level language consists of 332 sites (appendix~\ref{appendix:sites}) which are crucial to create posture suing various types of constraints. For example, the \emph{place} constraint requires a site to be placed at a specific location on the avatar's mesh. These sites are used to define the constraints that drive the avatar's movement and deformation during animation. In the previous low level synthesizor for AZee, these sites were represented as empty objects which had to be created manually in blender. This was a tedious time consuming process process and could lead to errors (figure~\ref{fig:prev_sites}).

\begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{chapters/rigging_layers/images/prev_sites.png}
    \caption{Manually created sites in the previous low level synthesizor for AZee}
    \label{fig:prev_sites}
\end{figure}

To automate this, we can use AZee's bone structure as reference as it remains consistent across avatars. For this, we use a \textbf{directional surface projection technique} (Figure~\ref{fig:site_surface_projection}), where points are projected from a bone's position in a specified direction to detect intersections with the avatar's surface. In this case, the projection is initiated from a bone's position within the avatar's mesh, and the intersection point is used to generate a site. This algorithm generates sites based on the bone's location and orientation, ensuring that the sites are accurately positioned on the avatar's mesh. Algorithm~\ref{alg:site_generation_with_projection} outlines the projection process for automatic site generation.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.5\textwidth]{chapters/rigging_layers/images/site_surface_projection.png}
    \caption{Projection example for site generation}
    \label{fig:site_surface_projection}
\end{figure}

\begin{algorithm}[ht]
    \caption{Projection Algorithm for Automatic Site Generation}
    \label{alg:site_generation_with_projection}
    \begin{algorithmic}
        \State \textbf{Input:} A 3D model with bones
        \State \textbf{Output:} Generated and positioned sites on the model

        \State $sites \gets$ an empty list to store generated site locations

        \State Remove existing sites from the model

        \State $armature \gets$ find the armature associated with the 3D model

        \For{each $bone$ in $armature$}
            \State $start\_point \gets$ determine the starting point based on the bone's position
            \State $direction \gets$ calculate the direction vector based on the bone's orientation
            
            \State $hit\_success, hit\_location \gets$ perform surface projection from $start\_point$ in $direction$
            \If{$hit\_success$}
                \State $site\_location \gets hit\_location$
            \Else
                \State $site\_location \gets$ fallback location based on predefined logic
            \EndIf

            \State Add $site\_location$ to the $sites$ list
            
            \State Apply any additional constraints or relationships between the site and the bone
        \EndFor

        \For{each $finger$ in the model}
            \For{each $segment$ in $finger$}
                \State Repeat the projection procedure to determine and store $site\_locations$ for each segment
            \EndFor
        \EndFor

        \State Finalize and store the $sites$ list for further processing or rendering
    \end{algorithmic}
\end{algorithm}

The generated sites can be visualized on the avatar in figure~\ref{fig:sites_bazeel_combined}.

\begin{figure}[h]
    \centering
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{chapters/rigging_layers/images/sites_body_front.png}
        \caption{Front View of Body Sites}
        \label{fig:sites_body_front}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{chapters/rigging_layers/images/sites_body_back.png}
        \caption{Back View of Body Sites}
        \label{fig:sites_body_back}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{chapters/rigging_layers/images/sites_hand.png}
        \caption{Hand Sites}
        \label{fig:sites_hand}
    \end{subfigure}
    \caption{AZee sites on the BAZeel avatar: Front and Back Body Views, and Hand Sites}
    \label{fig:sites_bazeel_combined}
\end{figure}

\subsection{Creating the bone heirarchy}
\label{ch:avatar_creation_pose_synthesis:proc_rig_signing_avatars:bone_layers}

The previous custom rig contained 57 bones (appendix~\ref{appendix:old_bone_list}). These bones were initially structured to accommodate the unique skeletal system requirements of the AZee framework, which utilizes a "rotation first" paradigm. However, since most 3D softwares operate on a "translation first" paradigm, a conversion process was necessary. This conversion involved mapping each AZee bone to two corresponding avatar bones: one for translation and one for rotation, with the rotation bone named by appending a ".rot" suffix to the original bone name (figure~\ref{fig:old_bone_structure}).

\begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{chapters/rigging_layers/images/old_bone_structure.png}
    \caption{Duplicate bones for rotation and translation}
    \label{fig:old_bone_structure}
\end{figure}

The problem with this system however, is that it doesn't accoutn for mesh deformity. To address this, we used bone layers to organize the bones into separate groups based on their function and purpose. Bone layers are often used in 3D animation softwares to organize and manage bones in a rig. By grouping bones into separate layers, animators can control which bones are visible and selectable at any given time, making it easier to work with complex rigs and animations. The new system has 3 types of bone layers: the deformation bone layer, the inverse kinematics (IK) layer, and the forward kinematics (FK) layer.

\subsection{Deformation Bone Layer}
\label{ch:avatar_creation_pose_synthesis:proc_rig_signing_avatars:deform_bone_layer}

The deformation bone layer(figure~\ref{fig:deform_layer}) is the foundation of the avatar's rig. It includes the bones that directly influence the mesh of the character, controlling how the skin deforms in response to movement. This layer is responsible for the overall shape and posture of the avatar, ensuring that the character's movements are smooth and realistic.

In this layer, the bones are typically organized into a hierarchy, with the root bone controlling the overall position and orientation of the avatar, and child bones controlling specific parts of the body, such as the limbs, spine, and head. Weight painting is used to determine how much influence each bone has over the surrounding mesh, allowing for precise control over how the character's skin deforms.

\begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{chapters/rigging_layers/images/deform_layer.png}
    \caption{Deformation bone layer}
    \label{fig:deform_layer}
\end{figure}

\subsection{Forward Kinematics Layer}
\label{ch:avatar_creation_pose_synthesis:proc_rig_signing_avatars:fk_layer}

The forward kinematics (FK) layer (figure~\ref{fig:fk_layer}) controls the rotation of bones in a hierarchical manner, where the rotation of a parent bone affects all its children. This layer is used for broader, more deliberate movements, such as head tilts, body leans, or full-body rotations.

FK provides animators with a high degree of control over each individual bone in the rig, allowing for precise adjustments to the character's pose. Unlike IK, where the position of the end effector is specified, FK requires the animator to manually rotate each bone in the chain, starting from the root and working down to the extremities. This can be more time-consuming but offers greater flexibility in achieving complex poses.

\begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{chapters/rigging_layers/images/fk_layer.png}
    \caption{FK bone layer}
    \label{fig:fk_layer}
\end{figure}

\subsection{Inverse Kinematics Layer}
\label{ch:avatar_creation_pose_synthesis:proc_rig_signing_avatars:ik_layer}

The inverse kinematics (IK) layer (figure~\ref{fig:ik_layer}) is responsible for positioning the avatar's fingers, limbs and spine by solving for the joint angles required to achieve a desired end-effector position.

A separate IK layer ensures that the FK configuration is not lost during constraint evaluation. If the posture is in IK mode i.e. is solving for a placement, the deform layers copy the orientations resolved by the IK layer.

\begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{chapters/rigging_layers/images/ik_layer.png}
    \caption{IK bone layer}
    \label{fig:ik_layer}
\end{figure}

\subsection{Constraint Based Posture Optimization}
\label{ch:avatar_creation_pose_synthesis:proc_rig_signing_avatars:cb_posegen}

The previous low level synthesizor for AZee~\cite{nunnari2018animating} used a constraint based optimization algorithm to generate the armature pose for a frame. This algorithm collects all posture constaints given by the AZee Animated Score for every frame and solves for the posture by optimizing these constraints using distance functions. 

However, their approach handles only placements and orientations and uses a flattened animated AZee Score. This was improved using the algorithm~\ref{alg:trimmed_multi_track_optimization} by generating from the multi-track synced score itself(more discussed in chapter~\ref{ch:multi_track}) with support for other constraints such as \emph{morph}, \emph{lookat} and \emph{trill}.

\begin{algorithm}
    \caption{Constraint-Based Optimization for Posture Synthesis for a Multi-Track Timeline}
    \label{alg:trimmed_multi_track_optimization}
    \begin{algorithmic}[1]
        \Require $\text{PostureConstraints} \ \mathit{posture\_constraint\_DAG}$, $\text{Object} \ \mathit{armature\_object}$, $\text{FrameRange} \ \mathit{frames}$
        \Ensure Armature object is posed according to the constraints
        
        \State \textbf{Initialize:} $i \gets 0$
        \State $distance\_threshold \gets \mathit{bpy.context.scene.azee\_ik\_site\_distance\_threshold}$, $angle\_threshold \gets \mathit{bpy.context.scene.azee\_ik\_bone\_rot\_angle\_threshold \times \pi / 180}$
        \State $max\_iterations \gets \mathit{bpy.context.scene.azee\_ik\_max\_iterations}$
        \State $\mathit{sorted\_constraints} \gets \mathit{posture\_constraint\_DAG.topological\_sort()}$
        \State $frames\_to\_keyframe \gets \textsc{DetermineFramesToKeyframe}(\mathit{frames}, \mathit{dynamic\_points})$
        
        \While{$i < max\_iterations$}
            \ForAll {$f \in \mathit{frames\_to\_keyframe}$}
                \State $\mathit{cumulative\_loss} \gets 0.0$
                \ForAll {$constraint \in \mathit{sorted\_constraints}$}
                    \State \textbf{Apply Constraint:} \textsc{ApplyConstraint}($constraint$, $f$)
                    \State $\mathit{cumulative\_loss} \gets cumulative\_loss + \mathit{constraint.get\_loss(f)}$
                \EndFor
                \If {$cumulative\_loss < \mathit{bpy.context.scene.azee\_animator\_threshold}$}
                    \State \textbf{Break:} \textbf{end while}
                \EndIf
                \State \textbf{Insert Keyframe:} \textsc{InsertKeyframe}($f$)
            \EndFor
            \State $i \gets i + 1$
        \EndWhile
        
        \Procedure{ApplyConstraint}{$constraint, frame$}
            \If {$constraint$ \textbf{is} PlacementConstraint}
                \State \textsc{PlaceSite}($constraint.site$, \textsc{EvaluatePlacementTarget}($constraint$, $frame$))
            \ElsIf {$constraint$ \textbf{is} OrientationConstraint}
                \State \textsc{RotateBone}($constraint.bone$, \textsc{EvaluateOrientation}($constraint$, $frame$))
            \ElsIf {$constraint$ \textbf{is} MorphConstraint}
                \State \textsc{ApplyMorph}($constraint$, $frame$)
            \ElsIf {$constraint$ \textbf{is} LookAtConstraint}
                \State \textsc{RotateEyesToTarget}($constraint.eyes$, \textsc{EvaluateLookAtPoint}($constraint$, $frame$))
            \ElsIf {$constraint$ \textbf{is} TrillConstraint}
                \State \textsc{ApplyTrill}($constraint$, $frame$)
            \ElsIf {$constraint$ \textbf{is} TranspathConstraint}
                \State \textsc{FollowPath}($constraint.site$, \textsc{DeterminePath}($constraint$, $frame$))
            \Else
                \State \textsc{HandleError}()
            \EndIf
        \EndProcedure
        
        \Procedure{DetermineFramesToKeyframe}{$frames, dynamic\_points$}
            \State \Return $\textsc{IdentifyKeyFrames}(\mathit{frames}) \cup \textsc{DynamicFrameRanges}(dynamic\_points)$
        \EndProcedure
        
        \Procedure{InsertKeyframe}{$frame$}
            \ForAll {$fcurve \in \mathit{armature\_object.animation\_data.action.fcurves}$}
                \State \textsc{KeyframeInsert}($fcurve$, $frame$)
            \EndFor
        \EndProcedure
    \end{algorithmic}
\end{algorithm}

To omit impossible poses, the skeleton is also assigned joint limit constraints. These constraints are used to prevent the avatar from entering physically impossible poses, such as extending or bending a joint beyond its natural range of motion. Figure~\ref{fig:joint_limits} shows an example of joint limits applied to the avatar's arm.

\begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{chapters/rigging_layers/images/joint_limits.png}
    \caption{Joint limits applied to the avatar's arm}
    \label{fig:joint_limits}
\end{figure}

\subsection{Animating Morphs Constraints}
\label{ch:avatar_creation_pose_synthesis:proc_rig_signing_avatars:morph_constraints}

The word "morph" comes from the Greek word \textit{morphē} (\textgreek{μορφή}), which means "form" or "shape." In modern usage, "morph" is often used as a shorthand for "morphing." In the field of computer graphics and animation, morphs are predefined shape keys that modify the mesh of the avatar independently of the skeletal structure, allowing for a fine control of the avatar's appearance that cannot be easily achieved through bone manipulation alone. This includes subtle facial expressions, muscle bulges, and other nuanced gestures that are critical for conveying meaning in \gls{sl}.

AZee didn't have any morph specification to control the avatar. Durign this work, we populated them with skeletal and facial morphs. These morphs are changes in the configuration of the avatar, which can be specified by the linguist in a range from 0 to 1. For instance, closing of hands(a change in skeleton configuration) or raising the eyebrows(a change in the 3D mesh). The syntax of a morph constraint is as follows:

\[
\texttt{morph } \text{\emph{'morph\_id'}} \ \texttt{weight[0, 1]}
\]

\subsubsection{Skeletal Morphs}
\label{ch:avatar_creation_pose_synthesis:proc_rig_signing_avatars:morph_constraints:skel_morphs}

Skeletal morphs are a specific type of morphs that directly manipulate the skeletal structure of an avatar to produce a desired pose or motion. Unlike traditional bone-based manipulations that rely solely on inverse or forward kinematics, skeletal morphs allow for more nuanced and localized control over specific joints or groups of joints. This is particularly useful for complex hand gestures or other detailed movements that are difficult to achieve with bone rotations alone.

In AZee, skeletal morphs are treated as constraints that define a forward kinematic change in the avatar's skeletal configuration. These morphs can influence various aspects of the skeleton, such as:

\begin{itemize}
    \item \textbf{Flexion and Extension:} This refers to the bending (flexion) or straightening (extension) of a joint (figure~\ref{fig:hyper-extension_flexion}), such as the fingers or wrists. Flexion reduces the angle between bones, while extension increases it.
    \item \textbf{Adduction and Abduction:} Adduction involves moving a limb closer to the body's midline (figure~\ref{fig:adduction_abduction}), whereas abduction moves it away. These movements are crucial for natural arm and hand gestures.
    \item \textbf{Hyperextension:} This is an extension of a joint beyond its normal range of motion (figure~\ref{fig:hyper-extension_flexion}), often used for expressive gestures that require exaggerated poses.
\end{itemize}

\begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{chapters/rigging_layers/images/adduction_abduction.jpg}
    \caption{Adduction and Abduction of the palm}
    \label{fig:adduction_abduction}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{chapters/rigging_layers/images/hyper-extension_flexion.jpg}
    \caption{Flextion and Hyperextension of the index finger}
    \label{fig:hyper-extension_flexion}
\end{figure}

Figure~\ref{fig:skeletal_morphs} shows a table of skeletal morphs and their corresponding effects on the avatar's skeleton.

todo add diagrams in the figure
\begin{figure}
    \centering
    \begin{tabular}{|c|c|}
        \hline
        \textbf{Morph ID} & \textbf{Effect} \\
        \hline
        % Finger Morphs
        \texttt{index\_bend\_L} & Bends the left index finger \\
        \texttt{middle\_bend\_L} & Bends the left middle finger \\
        \texttt{ring\_bend\_L} & Bends the left ring finger \\
        \texttt{little\_bend\_L} & Bends the left little finger \\
        \texttt{index\_bend\_R} & Bends the right index finger \\
        \texttt{middle\_bend\_R} & Bends the right middle finger \\
        \texttt{ring\_bend\_R} & Bends the right ring finger \\
        \texttt{little\_bend\_R} & Bends the right little finger \\
        \texttt{index\_hook\_L} & Hooks the left index finger \\
        \texttt{middle\_hook\_L} & Hooks the left middle finger \\
        \texttt{ring\_hook\_L} & Hooks the left ring finger \\
        \texttt{little\_hook\_L} & Hooks the left little finger \\
        \texttt{index\_hook\_R} & Hooks the right index finger \\
        \texttt{middle\_hook\_R} & Hooks the right middle finger \\
        \texttt{ring\_hook\_R} & Hooks the right ring finger \\
        \texttt{little\_hook\_R} & Hooks the right little finger \\
        \texttt{thumb\_bend\_L} & Bends the left thumb \\
        \texttt{thumb\_hook\_L} & Hooks the left thumb \\
        \texttt{thumb\_opposed\_L} & Moves the left thumb to an opposed position \\
        \texttt{thumb\_bend\_R} & Bends the right thumb \\
        \texttt{thumb\_hook\_R} & Hooks the right thumb \\
        \texttt{thumb\_opposed\_R} & Moves the right thumb to an opposed position \\
        \texttt{fingers\_spread\_L} & Spreads the fingers on the left hand \\
        \texttt{fingers\_spread\_R} & Spreads the fingers on the right hand \\
        \hline
        % Body Posture Morphs
        \texttt{shoulder\_raise\_L} & Raises the left shoulder \\
        \texttt{shoulder\_raise\_R} & Raises the right shoulder \\
        \texttt{shoulder\_shrug\_L} & Shrugs the left shoulder \\
        \texttt{shoulder\_shrug\_R} & Shrugs the right shoulder \\
        \texttt{spine\_extended} & Extends the spine \\
        \hline
    \end{tabular}
    \caption{Table of skeletal morphs and their effects}
    \label{fig:skeletal_morphs}
\end{figure}

\subsubsection{Facial Morphs}
\label{ch:avatar_creation_pose_synthesis:proc_rig_signing_avatars:morph_constraints:facial_morphs}

Facial morphs in the AZee framework provide a means to represent the complex and nuanced expressions essential for conveying emotions and grammatical cues using subtle facial movements that are integral to effective communication in \gls{sl} (figure~\ref{fig:facial_example}).

\begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{chapters/rigging_layers/images/facial_example.png}
    \caption{Facial morph for \emph{:inter-subjecitivty}}
    \label{fig:facial_example}
\end{figure}

Facial Morph targets are typically generated by sculpting the avatar's mesh into various shapes that correspond to different expressions or gestures. These sculpted shapes are then saved as "keys," which can be blended together during animation to produce the desired expression. This process allows for a high degree of flexibility in animation, as different keys can be combined in real-time to create a wide range of expressions. While this section offers a brief overview, the detailed modeling, implementation, and challenges associated with facial morphs are discussed extensively in Chapter~\ref{ch:facial_expressions}.

\subsubsection{Integration with other layers}
\label{ch:avatar_creation_pose_synthesis:proc_rig_signing_avatars:morph_constraints:intergation}

The morph constraints are integrated along with the other constraints in the posture optimization algorithm. Top optimize skeletal morph constraints we calculate the FK values of the impacted bones for skeletal morphs and calculate the loss using rotation distance formula.

\[
d_R = \min\left( 2 \cos^{-1}\left( \left| q_1 \cdot q_2 \right| \right), 2\pi - 2 \cos^{-1}\left( \left| q_1 \cdot q_2 \right| \right) \right)
\]

Where:
\begin{itemize}
    \item \( q_1 \) and \( q_2 \) are the quaternions representing the current and target bone rotations, respectively.
    \item \( q_1 \cdot q_2 \) is the dot product of the two quaternions.
    \item \( \cos^{-1} \) is the inverse cosine function.
    \item The absolute value \( | q_1 \cdot q_2 | \) ensures the angle is within the correct range (0 to \( \pi \)).
    \item The \( \min \) ensures we take the shortest rotation distance.
\end{itemize}

For the the facial morphs however, the morphs act on the mesh and the loss is calculated using the distance between the vertices.

\[
d_V = \frac{1}{N} \sum_{i=1}^{N} \| \mathbf{v}_i - \mathbf{v}_i' \|
\]

Where:
\begin{itemize}
    \item \( N \) is the number of vertices impacted by the morph.
    \item \( \mathbf{v}_i \) is the position of the \( i \)-th vertex in the original mesh.
    \item \( \mathbf{v}_i' \) is the position of the \( i \)-th vertex in the morphed mesh.
    \item \( \|\cdot\| \) denotes the Euclidean distance between corresponding vertices.
\end{itemize}

\section{Results and Implementation}
\label{ch:avatar_creation_pose_synthesis:results}

The layered rigging system has been implemented and tested on the BAZeel (MakeHuman) avatar in blender. The final avatar with layers as well as sites can be seen in figure~\ref{fig:layers_example}.

\begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{chapters/rigging_layers/images/layers_example.png}
    \caption{Example of the layered rigging system on the BAZeel avatar}
    \label{fig:layers_example}
\end{figure}

\section{Evaluation}
\label{ch:avatar_creation_pose_synthesis:evaluation}

We evaluate the performance, accuracy, and quality of the animation generated by the layered rigging system.

\subsection{Accuracy of the Animation}
\label{ch:avatar_creation_pose_synthesis:evaluation:accuracy}

The accuracy of the animation can be evaluated using the Frechet gesture distance(todo frobenius distance) calculated using the generated animation and a reference animation based on the AZee body sites. Table~\ref{tab:accuracy_metrics} shows the results of the evaluation.

\begin{table}
    \centering
    \begin{tabular}{|c|c|}
        \hline
        \textbf{Metric} & \textbf{Value} \\
        \hline
        Frechet Gesture Distance & todo \\
        Old synthesizor & todo \\
        Paula & todo \\
        Sgnify & todo \\
        Mocap(Retargeted) & todo \\
        \hline
    \end{tabular}
    \caption{Evaluation metrics for the accuracy of the animation}
    \label{tab:accuracy_metrics}
\end{table}

\subsection{Quality of Animation}
\label{ch:avatar_creation_pose_synthesis:evaluation:quality}

As observed, the generated animations are robotic in nature and lack the fluidity and expressiveness of natural \gls{sl} gestures. However, everything which can be annotated using the AZee low level, can be synthesized using this technique. This provides us with a good starting block to animate \gls{sl}. Moreover, with the procedural rigging system, we can us this same system with other avatars as well.

\subsection{Performance}
\label{ch:avatar_creation_pose_synthesis:evaluation:performance}

The animation time compared to the previous synthesizor are shown in table~\ref{tab:faster_executions}.

\begin{table}
    \centering
    \begin{tabular}{|c|c|}
        \hline
        \textbf{Interface} & \textbf{Execution time (s)} \\
        \hline
        Sign & Time ratio \\
        \emph{tree} & todo \\
        \emph{good} & todo \\
        \hline
    \end{tabular}
    \caption{Comparison of execution times between the old and new interfaces}
    \label{tab:faster_executions}
\end{table}

We observe that this synthesis is significantly faster because of direct integration with blender's armature. The automatic site generation and rigging also reduce the time taken to set up the avatar for animation.

\section{Conclusion}
\label{ch:avatar_creation_pose_synthesis:conclusion}

todo fix this
In this chapter, we have presented a layered rigging system for signing avatars that offers a more flexible and scalable approach to rigging and animation. By dividing the rigging process into distinct layers, each responsible for a different aspect of the avatar's movement and deformation, we can achieve a higher level of control and realism in the animations. The automatic site generation and rigging process significantly reduce the time and effort required to set up the avatar for animation, while the constraint-based posture optimization algorithm ensures that the avatar's movements are accurate and natural. The future work includes improving the quality of the animations, integrating facial expressions, and explores the use of machine learning techniques to enhance the realism and expressiveness of the synthesized discourse.

\end{document}