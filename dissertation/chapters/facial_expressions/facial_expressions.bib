@InProceedings{Forte_2023_CVPR,
    author    = {Forte, Maria-Paola and Kulits, Peter and Huang, Chun-Hao P. and Choutas, Vasileios and Tzionas, Dimitrios and Kuchenbecker, Katherine J. and Black, Michael J.},
    title     = {Reconstructing Signing Avatars From Video Using Linguistic Priors},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2023},
    pages     = {12791-12801}
}

@inproceedings{johnson-2022-improved,
    title = "Improved Facial Realism through an Enhanced Representation of Anatomical Behavior in Sign Language Avatars",
    author = "Johnson, Ronan",
    editor = "Efthimiou, Eleni  and
      Fotinea, Stavroula-Evita  and
      Hanke, Thomas  and
      McDonald, John C.  and
      Shterionov, Dimitar  and
      Wolfe, Rosalee",
    booktitle = "Proceedings of the 7th International Workshop on Sign Language Translation and Avatar Technology: The Junction of the Visual and the Textual: Challenges and Perspectives",
    month = jun,
    year = "2022",
    address = "Marseille, France",
    publisher = "European Language Resources Association",
    url = "https://aclanthology.org/2022.sltat-1.8",
    pages = "53--58",
    abstract = "Facial movements and expressions are critical features of signed languages, yet are some of the most challenging to reproduce on signing avatars. Due to the relative lack of research efforts in this area, the facial capabilities of such avatars have yet to receive the approval of those in the Deaf community. This paper revisits the representations of the human face in signed avatars, specifically those based on parameterized muscle simulation such as FACS and the MPEG-4 file definition. An improved framework based on rotational pivots and pre-defined movements is capable of reproducing realistic, natural gestures and mouthings on sign language avatars. The new approach is more harmonious with the underlying construction of signed avatars, generates improved results, and allows for a more intuitive workflow for the artists and animators who interact with the system.",
}

@article{azevedo2024empowering,
  title={Empowering Sign Language Communication: Integrating Sentiment and Semantics for Facial Expression Synthesis},
  author={Azevedo, Rafael and Coutinho, Thiago and Ferreira, Jo{\~a}o and Gomes, Thiago and Nascimento, Erickson},
  journal={arXiv preprint arXiv:2408.15159},
  year={2024}
}

@inproceedings{danvevcek2022emoca,
  title={Emoca: Emotion driven monocular face capture and animation},
  author={Dan{\v{e}}{\v{c}}ek, Radek and Black, Michael J and Bolkart, Timo},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={20311--20322},
  year={2022}
}

@article{luo2022learning,
  title={Learning multi-dimensional edge feature-based au relation graph for facial action unit recognition},
  author={Luo, Cheng and Song, Siyang and Xie, Weicheng and Shen, Linlin and Gunes, Hatice},
  journal={arXiv preprint arXiv:2205.01782},
  year={2022}
}

@article{ekman1978facial,
  title={Facial action coding system},
  author={Ekman, Paul and Friesen, Wallace V},
  journal={Environmental Psychology \& Nonverbal Behavior},
  year={1978}
}

@inproceedings{challant2024extending,
  title={Extending AZee with Non-manual Gesture Rules for French Sign Language},
  author={Challant, Camille and Filhol, Michael},
  booktitle={2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)},
  pages={7007--7016},
  year={2024},
  organization={European Language Resources Association}
}

@article{gilbert2021facshuman,
  title={FACSHuman, a software program for creating experimental material by modeling 3D facial expressions},
  author={Gilbert, Micha{\"e}l and Demarchi, Samuel and Urdapilleta, Isabel},
  journal={Behavior Research Methods},
  volume={53},
  number={5},
  pages={2252--2272},
  year={2021},
  publisher={Springer}
}


@article{you_said_that,
  author    = {Amir Jamaludin and
               Joon Son Chung and
               Andrew Zisserman},
  title     = {You Said That?: Synthesising Talking Faces from Audio},
  journal   = {Int. J. Comput. Vis.},
  volume    = {127},
  number    = {11-12},
  pages     = {1767--1779},
  year      = {2019},
  url       = {https://doi.org/10.1007/s11263-019-01150-y},
  doi       = {10.1007/s11263-019-01150-y},
  timestamp = {Fri, 13 Mar 2020 00:00:00 +0100},
  biburl    = {https://dblp.org/rec/journals/ijcv/JamaludinCZ19.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{talking_face,
  title={Talking Face Generation by Conditional Recurrent Adversarial Network},
  author={Yang Song and Jingwen Zhu and Dawei Li and Xiaolong Wang and Hairong Qi},
  booktitle={IJCAI},
  year={2019}
}

@article{lip_movements,
  title={Lip Movements Generation at a Glance},
  author={Lele Chen and Zhiheng Li and Ross K. Maddox and Zhiyao Duan and Chenliang Xu},
  journal={ArXiv},
  year={2018},
  volume={abs/1803.10404}
}

@inbook{lip_sync_expert,
author = {Prajwal, K R and Mukhopadhyay, Rudrabha and Namboodiri, Vinay P. and Jawahar, C.V.},
title = {A Lip Sync Expert Is All You Need for Speech to Lip Generation In the Wild},
year = {2020},
isbn = {9781450379885},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3394171.3413532},
abstract = {In this work, we investigate the problem of lip-syncing a talking face video of an arbitrary identity to match a target speech segment. Current works excel at producing accurate lip movements on a static image or videos of specific people seen during the training phase. However, they fail to accurately morph the lip movements of arbitrary identities in dynamic, unconstrained talking face videos, resulting in significant parts of the video being out-of-sync with the new audio. We identify key reasons pertaining to this and hence resolve them by learning from a powerful lip-sync discriminator. Next, we propose new, rigorous evaluation benchmarks and metrics to accurately measure lip synchronization in unconstrained videos. Extensive quantitative evaluations on our challenging benchmarks show that the lip-sync accuracy of the videos generated by our Wav2Lip model is almost as good as real synced videos. We provide a demo video clearly showing the substantial impact of our Wav2Lip model, and also publicly release the code, models, and evaluation benchmarks on our website.},
booktitle = {Proceedings of the 28th ACM International Conference on Multimedia},
pages = {484–492},
numpages = {9}
}


@inbook{imitating,
author = {Wu, Haozhe and Jia, Jia and Wang, Haoyu and Dou, Yishun and Duan, Chao and Deng, Qingshan},
title = {Imitating Arbitrary Talking Style for Realistic Audio-Driven Talking Face Synthesis},
year = {2021},
isbn = {9781450386517},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3474085.3475280},
abstract = {People talk with diversified styles. For one piece of speech, different talking styles exhibit significant differences in the facial and head pose movements. For example, the "excited" style usually talks with the mouth wide open, while the "solemn" style is more standardized and seldomly exhibits exaggerated motions. Due to such huge differences between different styles, it is necessary to incorporate the talking style into audio-driven talking face synthesis framework. In this paper, we propose to inject style into the talking face synthesis framework through imitating arbitrary talking style of the particular reference video. Specifically, we systematically investigate talking styles with our collected Ted-HD dataset and construct style codes as several statistics of 3D morphable model (3DMM) parameters. Afterwards, we devise a latent-style-fusion (LSF) model to synthesize stylized talking faces by imitating talking styles from the style codes. We emphasize the following novel characteristics of our framework: (1) It doesn't require any annotation of the style, the talking style is learned in an unsupervised manner from talking videos in the wild. (2) It can imitate arbitrary styles from arbitrary videos, and the style codes can also be interpolated to generate new styles. Extensive experiments demonstrate that the proposed framework has the ability to synthesize more natural and expressive talking styles compared with baseline methods.},
booktitle = {Proceedings of the 29th ACM International Conference on Multimedia},
pages = {1478–1486},
numpages = {9}
}

@article{cudeiro,
  title={Capture, Learning, and Synthesis of 3D Speaking Styles},
  author={Daniel Cudeiro and Timo Bolkart and Cassidy Laidlaw and Anurag Ranjan and Michael J. Black},
  journal={2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2019},
  pages={10093-10103}
}

% FLAME
@inproceedings{FLAME,
  added-at = {2018-11-06T00:00:00.000+0100},
  author = {Wu, Yao and Ester, Martin},
  biburl = {https://www.bibsonomy.org/bibtex/29112be2dff89053b24a898251a888f92/dblp},
  booktitle = {WSDM},
  editor = {Cheng, Xueqi and Li, Hang and Gabrilovich, Evgeniy and Tang, Jie},
  ee = {https://doi.org/10.1145/2684822.2685291},
  interhash = {674401e807a00a82497444214dd2d8af},
  intrahash = {9112be2dff89053b24a898251a888f92},
  isbn = {978-1-4503-3317-7},
  keywords = {dblp},
  pages = {199-208},
  publisher = {ACM},
  timestamp = {2019-05-22T11:54:34.000+0200},
  title = {FLAME: A Probabilistic Model Combining Aspect Based Opinion Mining and Collaborative Filtering.},
  year = 2015
}

@article{Yang:2020:MakeItTalk,
	Author    = {Yang Zhou and Xintong Han and Eli Shechtman and Jose Echevarria and Evangelos Kalogerakis and Dingzeyu Li},
	Title     = {MakeItTalk: Speaker-Aware Talking-Head Animation},
	Journal   = {ACM Transactions on Graphics},
	Volume    = {39},
	Number    = {6},
	Year      = {2020},
}


@inproceedings{eamm,
author = {Ji, Xinya and Zhou, Hang and Wang, Kaisiyuan and Wu, Qianyi and Wu, Wayne and Xu, Feng and Cao, Xun},
title = {EAMM: One-Shot Emotional Talking Face via Audio-Based Emotion-Aware Motion Model},
year = {2022},
isbn = {9781450393379},
url = {https://doi.org/10.1145/3528233.3530745},
doi = {10.1145/3528233.3530745},
booktitle = {ACM SIGGRAPH 2022 Conference Proceedings},
series = {SIGGRAPH '22}
}

@inproceedings{challant2022first,
  title={A first corpus of azee discourse expressions},
  author={Challant, Camille and Filhol, Michael},
  booktitle={Language Resources and Evaluation Conference},
  year={2022}
}