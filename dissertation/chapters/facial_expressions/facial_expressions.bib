@InProceedings{Forte_2023_CVPR,
    author    = {Forte, Maria-Paola and Kulits, Peter and Huang, Chun-Hao P. and Choutas, Vasileios and Tzionas, Dimitrios and Kuchenbecker, Katherine J. and Black, Michael J.},
    title     = {Reconstructing Signing Avatars From Video Using Linguistic Priors},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2023},
    pages     = {12791-12801}
}

@inproceedings{johnson-2022-improved,
    title = "Improved Facial Realism through an Enhanced Representation of Anatomical Behavior in Sign Language Avatars",
    author = "Johnson, Ronan",
    editor = "Efthimiou, Eleni  and
      Fotinea, Stavroula-Evita  and
      Hanke, Thomas  and
      McDonald, John C.  and
      Shterionov, Dimitar  and
      Wolfe, Rosalee",
    booktitle = "Proceedings of the 7th International Workshop on Sign Language Translation and Avatar Technology: The Junction of the Visual and the Textual: Challenges and Perspectives",
    month = jun,
    year = "2022",
    address = "Marseille, France",
    publisher = "European Language Resources Association",
    url = "https://aclanthology.org/2022.sltat-1.8",
    pages = "53--58",
    abstract = "Facial movements and expressions are critical features of signed languages, yet are some of the most challenging to reproduce on signing avatars. Due to the relative lack of research efforts in this area, the facial capabilities of such avatars have yet to receive the approval of those in the Deaf community. This paper revisits the representations of the human face in signed avatars, specifically those based on parameterized muscle simulation such as FACS and the MPEG-4 file definition. An improved framework based on rotational pivots and pre-defined movements is capable of reproducing realistic, natural gestures and mouthings on sign language avatars. The new approach is more harmonious with the underlying construction of signed avatars, generates improved results, and allows for a more intuitive workflow for the artists and animators who interact with the system.",
}

@article{azevedo2024empowering,
  title={Empowering Sign Language Communication: Integrating Sentiment and Semantics for Facial Expression Synthesis},
  author={Azevedo, Rafael and Coutinho, Thiago and Ferreira, Jo{\~a}o and Gomes, Thiago and Nascimento, Erickson},
  journal={arXiv preprint arXiv:2408.15159},
  year={2024}
}

@inproceedings{danvevcek2022emoca,
  title={Emoca: Emotion driven monocular face capture and animation},
  author={Dan{\v{e}}{\v{c}}ek, Radek and Black, Michael J and Bolkart, Timo},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={20311--20322},
  year={2022}
}

@article{luo2022learning,
  title={Learning multi-dimensional edge feature-based au relation graph for facial action unit recognition},
  author={Luo, Cheng and Song, Siyang and Xie, Weicheng and Shen, Linlin and Gunes, Hatice},
  journal={arXiv preprint arXiv:2205.01782},
  year={2022}
}

@article{ekman1978facial,
  title={Facial action coding system},
  author={Ekman, Paul and Friesen, Wallace V},
  journal={Environmental Psychology \& Nonverbal Behavior},
  year={1978}
}