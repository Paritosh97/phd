\documentclass[../../main.tex]{subfiles}
\usepackage{tikz}
\usetikzlibrary{positioning}
\begin{document}
\chapter{Motion Matching for Sign Language Synthesis}
\label{ch:pose_correction}

In the previous chapters, we looked at granularity in sign language animation based on AZee's (linguistic) structure. However, one of the key missing pieces in our study was the ability to shortcut or match a pose on already seen poses. In video games, Motion matching is a technique that dynamically selects the most suitable animation frame or pose from a pre-recorded database based on current input constraints to create fluid, realistic motion. By comparing the motion of the character to a database of motion data, motion matching can generate more realistic and contextually appropriate animations.

The past decade has seen a rise in deep learning technologies in in the field of character animation. These technologies have been used to generate realistic and expressive animations for a wide range of applications, from video games to virtual reality. One of the key challenges in character animation is the generation of natural poses, which requires the ability to capture the nuances of human movement and behavior. 

In this work, we focus on the development of a pose corrector module in our existing synthesis system. We train a pose prior model using a French Sign Language motion capture dataset. The pose prior model captures the typical poses and movements associated with different signs, providing a statistical framework that guides the motion matching process. By learning these priors from a large dataset, our system is able to generate realistic and contextually appropriate sign language poses.

In this chapter, section~\ref{ch:pose_correction:related_work} discusses relevant background work in motion matching and pose correction, focusing on classical methods, data-driven approaches, and the integration of latent space representations. Section~\ref{ch:pose_correction:pose_correction_with_azee} presents the application of thi to the AZee low-level synthesis system, including the preparation of the dataset, training of the Variational Auto Encoder, implementation of pose correction, results and evaluation. Finally, section~\ref{ch:pose_correction:discussion} discusses the implications of integrating pose correction into the AZee system and outlines future directions for research.

\section{Related work}
\label{ch:pose_correction:related_work}

In this section, we discuss relevant background work in generating natural poses, focusing on classical methods, data-driven approaches, and the integration of latent space representations. These methodologies form the foundation for understanding advancements in character animation.

\subsection{Classical Inverse Kinematics (IK) Methods}
\label{ch:pose_correction:related_work:classical_ik}

Classical Inverse Kinematics (IK) methods have been a cornerstone in the field of character animation, providing techniques to determine the necessary joint angles to achieve a desired end-effector position. Some of these methods are Jacobian-based approach~\cite{4648032}, Cyclic Coordinate Descent (CCD)\cite{kenwright2012inverse}, and Forward And Backward Reaching Inverse Kinematics (FABRIK)\cite{aristidou2011fabrik}.

Jacobian-Based Methods involve computing the Jacobian matrix to linearize the relationship between joint angles and end-effector positions. By iteratively adjusting joint angles to reduce the error between the current and target positions, these methods offer a robust solution for real-time applications\ref{fig:jacobian_based}. However, they often are sensitive to singularities, resulting in unrealistic or unstable motion.

\begin{figure}
    \centering \includegraphics[width = 2.5in]{chapters/pose_correction/images/jacobian_based.png}
    \caption{Jacobian based IK solving(approximation of the first derivative)}
    \label{fig:jacobian_based}
\end{figure}

Cyclic Coordinate Descent (CCD) simplifies the IK problem by iteratively adjusting one joint at a time, minimizing the distance between the end-effector and the target\ref{fig:ccdik}. CCD is computationally efficient and easy to implement, making it popular in game engines. However, its greedy approach can lead to suboptimal solutions, especially in highly constrained scenarios.

\begin{figure}
  \centering \includegraphics[width = 2.5in]{chapters/pose_correction/images/ccdik.png}
  \caption{Cyclic Coordinate Descent (CCD) IK solving(changes rotation of a joint - one at a time)}
  \label{fig:ccdik}
\end{figure}

FABRIK differs from other methods by focusing on the positions of joints rather than their angles. It iteratively adjusts the positions of joints through a two-pass approach, first from the end-effector to the root and then from the root to the end-effector\ref{fig:fabrik}. This method is known for its simplicity and stability, particularly in scenarios where maintaining a natural joint configuration is crucial.

\begin{figure}
  \centering \includegraphics[width = 2.5in]{chapters/pose_correction/images/fabrik.png}
  \caption{Forward And Backward Reaching Inverse Kinematics(FABRIK) solving(updates coordinates in 2 passes)}
  \label{fig:fabrik}
\end{figure}

These classical methods have been widely adopted due to their balance between computational efficiency and control, making them suitable for a variety of real-time applications. However, they have notable limitations. They are prone to singularities, where solutions become unstable, and often require significant computational resources, especially in cases of overlapping or long chains. The resulting movements can sometimes be unnatural or biomechanically unrealistic, particularly when handling joint limits or multiple end-effectors. Additionally, these methods struggle with complex constraints and often produce suboptimal solutions that may not adapt well to varying scenarios, making them less suitable for dynamic or highly detailed applications~\ref{fig:problems_classical}.

\begin{figure}
  \centering \includegraphics[width = 2.5in]{chapters/pose_correction/images/problems_classical.png}
  \caption{Problems with classical IK solving methods}
  \label{fig:problems_classical}
\end{figure}

\subsection{Data-Driven IK Approaches}
\label{ch:pose_correction:related_work:data_driven_ik}

The limitations of classical IK methods have spurred the development of data-driven approaches, which leverage large datasets and machine learning to improve the realism and flexibility of character animations.

Motion Matching represents a significant shift from traditional techniques by utilizing a large database of motion capture (mocap) data. Instead of predefined animation clips, Motion Matching dynamically selects the most appropriate pose based on user inputs and contextual parameters. This approach was notably employed by Ubisoft in the game \emph{For Honor}, where it enabled more fluid and responsive character animations~\ref{fig:for_honor}. Motion Matching's ability to break down animations into fine-grained clips allows for seamless transitions and more natural movements.

\begin{figure}
  \centering \includegraphics[width = 2.5in]{chapters/pose_correction/images/for_honor.png}
  \caption{Motion Matching in For Honor}
  \label{fig:for_honor}
\end{figure}

Phase-Functioned Neural Networks (PFNN) extend the capabilities of motion matching by incorporating phase information into the neural network's weights~\cite{10.1145/3072959.3073663} (figure~\ref{fig:pfnn}). This phase-aware approach allows the network to generate contextually appropriate animations that account for the cyclical nature of bipedal movement, such as walking or running. Unlike traditional methods that rely on blending animation clips, PFNN encodes the entire animation process within the neural network, offering greater flexibility and control.

\begin{figure}
  \centering \includegraphics[width = 2.5in]{chapters/pose_correction/images/pfnn.png}
  \caption{Phase-Functioned Neural Networks (PFNN) for motion matching}
  \label{fig:pfnn}
\end{figure}

Style-Based Inverse Kinematics (Style IK) (figure~\ref{fig:style_ik}) leverages machine learning to represent poses in a latent space, where the distribution of poses can be learned and sampled. Using Scaled Gaussian Process Latent Variable Models (SGPLVM), Style IK can generate stylized animations that conform to specific aesthetic or functional constraints. This approach is particularly useful for creating animations that need to adhere to a particular style or where mocap data is not available~\cite{grochow2004style}.

\begin{figure}
  \centering \includegraphics[width = 2.5in]{chapters/pose_correction/images/style_ik.png}
  \caption{Pose in latent space using Style IK}
  \label{fig:style_ik}
\end{figure}

These data-driven approaches represent a significant advancement over classical IK methods, offering greater flexibility, realism, and the ability to handle complex, non-linear constraints. However, they also introduce new challenges, such as the need for extensive training data and increased computational demands, particularly in real-time applications.

\subsection{Latent Space Representations in Animation}
\label{ch:pose_correction:related_work:latent_space}

Latent space representations have become increasingly important in character animation, providing a powerful tool for managing the complexity of pose and motion data. Variational Autoencoders (VAEs)\cite{kingma2013auto}, such as the one used in SMPLify-X\cite{pavlakos2019expressive}, learn a probabilistic model of human poses, allowing for the generation and manipulation of poses in a lower-dimensional space. In the context of pose estimation, VAEs help in predicting 3D poses from 2D images by learning a latent space that captures the distribution of plausible human poses. This latent space can then be sampled to generate realistic poses that meet specific constraints, such as end-effector positions or overall body posture\ref{fig:simplifyx}.

\begin{figure}
  \centering \includegraphics[width = 2.5in]{chapters/pose_correction/images/simplifyx.png}
  \caption{SMPLify-X}
  \label{fig:simplifyx}
\end{figure}

The use of latent space not only reduces the dimensionality of the data, making it easier to work with, but also enables more sophisticated manipulations, such as style transfer or the synthesis of new poses that blend characteristics from multiple examples. This has proven particularly valuable in scenarios where high-quality mocap data is not available, or where the goal is to create stylized or non-standard animations.

VPoser\cite{pavlakos2019expressive}, a specific implementation of a VAE, further demonstrates the power of latent space in animation. By encoding poses into a low-dimensional latent space, VPoser can be used in optimization processes to ensure that generated poses are both realistic and meet specific criteria. This approach is particularly effective in applications like pose estimation from monocular images, where the latent space helps to regularize the solution and avoid physically implausible poses.

Overall, latent space representations have become a crucial component of modern character animation, enabling more complex and realistic animations while also providing tools for creative control and manipulation.

\section{Motion Matching with AZee Low Level synthesis}
\label{ch:pose_correction:pose_correction_with_azee}

In this section, we discuss the application of motion matching techniques to the AZee low-level synthesis system. By integrating motion matching, data-driven IK, and latent space representations into the AZee framework, we aim to enhance the realism and expressiveness of sign language animations.

\subsection{Preparing the dataset}
\label{ch:pose_correction:pose_correction_with_azee:dataset}

The first step in integrating motion matching into AZee is to train a Variational Autoencoder (VAE) on set of sign language poses. For this task, we use a dataset of mocap data collected from the Rosetta dataset~\cite{bertin2022rosetta}. The dataset consists of 167066 poses and should capture the diversity of poses and movements associated with different signs, providing a rich source of training data for the VAE.. Since our focus was only on the upper body, we didn't use the facial bones and the lower body for the training. We also retargeted the mocap data to the BAZeel avatar, which is compatible with the AZee skeleton structure (figure~\ref{fig:retargeted}).

\begin{figure}
  \centering \includegraphics[width = 2.5in]{chapters/pose_correction/images/retargeted.png}
  \caption{Retargeted mocap data to BAZeel avatar}
  \label{fig:retargeted}
\end{figure}

Next, we converted the motion into AZee's FK pose array. The FK pose array consists of the rotation values of each joint in the AZee skeleton (figure~\ref{fig:azee_fk_pose}). This representation is more suitable for the VAE training process, as it captures the pose information in a compact and standardized format.

\begin{figure}
  \centering \includegraphics[width = 2.5in]{chapters/pose_correction/images/azee_fk_pose.png}
  \caption{AZee FK Pose Array}
  \label{fig:azee_fk_pose}
\end{figure}

\subsection{Training VPoser}
\label{ch:pose_correction:pose_correction_with_azee:training}

A VAE is a type of generative model that learns a low-dimensional latent space representation of the input data. In the context of character animation, a VAE can be used to capture the distribution of poses in a dataset, allowing for the generation of new poses that are statistically similar to the training data. The VAE consists of an encoder network that maps input poses to a latent space and a decoder network that reconstructs the input poses from the latent space (figure~\ref{fig:vae}).


\begin{figure}[h]
\centering
\begin{tikzpicture}[
    every neuron/.style={
        circle,
        draw,
        minimum size=0.6cm
    },
    neuron missing/.style={
        draw=none, 
        scale=1,
        text height=0.333cm,
        execute at begin node=\color{black}$\vdots$
    },
    layer/.style={
        rectangle,
        draw,
        text centered,
        minimum width=2cm
    }
]

% Input Layer
\foreach \i in {1,2,3}
    \node [every neuron/.try, neuron \i/.try] (input-\i) at (0,1.5-\i*0.6) {};

\node at (0,-1) {\vdots};
\node at (0,-1.5) {54};

% Encoder L1
\foreach \i in {1,2}
    \node [every neuron/.try, neuron \i/.try] (L1-\i) at (2,1.2-\i*0.6) {};

\node at (2,-1) {\vdots};
\node at (2,-1.5) {128};

% Encoder L2
\foreach \i in {1,2}
    \node [every neuron/.try, neuron \i/.try] (L2-\i) at (4,1.2-\i*0.6) {};

\node at (4,-1) {\vdots};
\node at (4,-1.5) {64};

% Latent Layer
\foreach \i in {1,2}
    \node [every neuron/.try, neuron \i/.try] (L3-\i) at (6,1.2-\i*0.6) {};

\node at (6,-1) {\vdots};
\node at (6,-1.5) {32};

% Decoder L4
\foreach \i in {1,2}
    \node [every neuron/.try, neuron \i/.try] (L4-\i) at (8,1.2-\i*0.6) {};

\node at (8,-1) {\vdots};
\node at (8,-1.5) {64};

% Decoder L5
\foreach \i in {1,2}
    \node [every neuron/.try, neuron \i/.try] (L5-\i) at (10,1.2-\i*0.6) {};

\node at (10,-1) {\vdots};
\node at (10,-1.5) {128};

% Output Layer
\foreach \i in {1,2,3}
    \node [every neuron/.try, neuron \i/.try] (output-\i) at (12,1.5-\i*0.6) {};

\node at (12,-1) {\vdots};
\node at (12,-1.5) {54};

% Draw the connections
\foreach \i in {1,2,3}
    \foreach \j in {1,2}
        \draw [->] (input-\i) -- (L1-\j);

\foreach \i in {1,2}
    \foreach \j in {1,2}
        \draw [->] (L1-\i) -- (L2-\j);

\foreach \i in {1,2}
    \foreach \j in {1,2}
        \draw [->] (L2-\i) -- (L3-\j);

\foreach \i in {1,2}
    \foreach \j in {1,2}
        \draw [->] (L3-\i) -- (L4-\j);

\foreach \i in {1,2}
    \foreach \j in {1,2}
        \draw [->] (L4-\i) -- (L5-\j);

\foreach \i in {1,2}
    \foreach \j in {1,2,3}
        \draw [->] (L5-\i) -- (output-\j);

\end{tikzpicture}
\caption{Architecture of the VAE}
\label{fig:vae}
\end{figure}

Trained on a dataset containing 167,066 poses. The dataset is divided into training, validation, and test sets, which are loaded using the \texttt{AnimationDS} class from the dataloader. During training, batches of poses are processed, with each batch consisting of 512 samples. The training is carried out on a CUDA-enabled GPU, utilizing mixed precision training for efficiency. The VPoser model architecture consists of a latent space with 32 dimensions (\texttt{latentD}), and the neural network has 512 neurons per layer. 

The loss function used during training includes two primary components: 
\begin{itemize}
    \item \textbf{Reconstruction Loss}: This loss is computed at the joint level by comparing the reconstructed pose with the original pose using L1 loss. An additional pose-level reconstruction loss (L2 loss) is applied during the first 10 epochs to help the model learn better early on.
    \item \textbf{KL Divergence Loss}: The KL divergence regularizes the latent space by enforcing it to follow a standard normal distribution, ensuring that the latent space representation is smooth and continuous.
\end{itemize}

The total loss is the sum of the reconstruction loss and KL divergence loss. The optimization is carried out using the Adam optimizer with a learning rate of $1 \times 10^{-2}$ and weight decay of $0.0001$. A learning rate scheduler is used, which reduces the learning rate by a factor of 0.5 every third of the training epochs. 

\subsection{Implementing Motion Matching}
\label{ch:pose_correction:pose_correction_with_azee:implementation}

With the VAE trained, we can now implement a motion matching system that leverages the learned latent space to match poses generated by the AZee system to the most appropriate pose in the dataset (figure~\ref{fig:pose_correction}).

\begin{figure}
  \centering \includegraphics[width = 2.5in]{chapters/pose_correction/images/pose_correction.png}
  \caption{Motion Matching with VAE}
  \label{fig:pose_correction}
\end{figure}

Algorithm~\ref{alg:pose_correction} outlines the motion matching process. Given a target pose generated by the AZee synthesizor, we first encode the pose into the latent space using the VPoser encoder. We then compute the distance between the encoded pose and each pose in the dataset, selecting the pose with the smallest distance as the best match. Finally, we decode the matched pose back into the AZee FK pose array and apply it to the character.

\begin{algorithm}
  \caption{AZee constraint optimization with motion matching algorithm}
  \label{alg:pose_correction}
  \begin{algorithmic}[1]
  \For{$frame$ \textbf{in} frames}
      \State \texttt{switch\_cursor\_to\_frame($f$)}
      \For{\texttt{parallel\_block \textbf{in} self.parallel\_blocks}}
          \State \texttt{constraints.add(parallel\_block.constraints)}
      \EndFor
      \For{\texttt{constraint \textbf{in} constraints}}
          \State \texttt{constraint.apply($frame$)}
      \EndFor
      \State \texttt{model.pose\_embedding}
      \State \texttt{model.global\_trans}
      \State \texttt{optimizer = \dots}
      \For{\texttt{epoch \textbf{in} range(max\_epochs)}}
          \State \texttt{optimizer.zero\_grad()}
          \State \texttt{\dots}
          \State \texttt{optimizer.step()}
          \If{\texttt{loss.item() < threshold}} \State \texttt{break} \EndIf
      \EndFor
      \State \texttt{posture.keyframe($frame$)}
  \EndFor
  \end{algorithmic}
  \end{algorithm}

\subsection{Results and Evaluation}
\label{ch:pose_correction:pose_correction_with_azee:results}

Snapshots with standard synthesis and motion matching synthesis and the corresponding AZee code for the same are shown in table~\ref{tab:results}.

\begin{table}
  \centering
  \begin{tabular}{|c|c|c|}
    \hline
    \textbf{Standard Synthesis} & \textbf{Motion Matching Synthesis} & \textbf{AZee Code} \\
    \hline
    \includegraphics[width = 1.5in]{chapters/pose_correction/images/standard_synthesis.png} & \includegraphics[width = 1.5in]{chapters/pose_correction/images/pose_correction_synthesis.png} & \begin{lstlisting}
      AZeePose pose = AZeeSynthesize();
      AZeeMotionMatch(pose);
    \end{lstlisting} \\
    \hline
  \end{tabular}
  \caption{Results of Motion Matching Synthesis}
  \label{tab:results}
\end{table}

The synthesized videos can also be viewed at \href{todo}.

Initial subjective evaluations suggest that the motion matching system produces more natural and contextually appropriate animations compared to standard synthesis. However, due to retargeting losses, the integration of motion matching into Sign Language synthesis is still in the early stages. We also observe that the system can change the pose of the character in a way that is not always desirable~\ref{fig:problem_pose_correction}.

\begin{figure}
  \centering \includegraphics[width = 2.5in]{chapters/pose_correction/images/problem_pose_correction.png}
  \caption{Problems with motion matching}
  \label{fig:problem_pose_correction}
\end{figure}

Lastly, figure~\ref{fig:losses} shows how retargeting the mocap data to the AZee skeleton structure results in a loss of information. This loss can affect the quality of the generated animations and is an area for future improvement.

\begin{figure}
  \centering \includegraphics[width = 2.5in]{chapters/pose_correction/images/losses.png}
  \caption{Losses in retargeting mocap data}
  \label{fig:losses}
\end{figure}

\section{Discussion}
\label{ch:pose_correction:discussion}

The integration of motion matching into the AZee system represents a significant advancement in sign language synthesis. By leveraging data-driven IK and latent space representations, we are able to generate more realistic and contextually appropriate sign language animations. This has the potential to enhance the expressiveness and naturalness of sign language avatars, making them more engaging and accessible to users. In some ways, this offfers a new perspective to Sign Language synthesis where the linguistics decides the "what" and the motion matching decides the "how" of the animation.

For future, posers based on neural distance fields~\cite{tiwari2022pose} or diffusion~\cite{lu2023dposer} could be used for motion matching since the current poser has a bayesian bias. Also, continuity of motion matching with respect to signing spaces could be studied further improving the the learnt pose prior.

While the integration of machine learning into SL synthesis offers numerous advantages, it also introduces new challenges. Data-driven and latent space methods typically require significant computational resources, both during training and inference. This can be a major barrier in real-time applications, where low latency is critical. Also the effectiveness of machine learning models depends heavily on the availability of high-quality training data. In many cases, obtaining sufficient mocap data can be difficult, especially for non-standard or stylized animations. Lastly, while machine learning models can generate realistic and high-quality animations, they often lack the fine-grained control that human animators require. Ensuring that these models produce outputs that align with artistic vision remains a significant challenge.

\end{document}