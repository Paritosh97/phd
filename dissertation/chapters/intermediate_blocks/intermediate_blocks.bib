@article{witkin1988spacetime,
  title={Spacetime constraints},
  author={Witkin, Andrew and Kass, Michael},
  journal={ACM Siggraph Computer Graphics},
  volume={22},
  number={4},
  pages={159--168},
  year={1988},
  publisher={ACM New York, NY, USA}
}

@article{10.1145/566654.566607,
author = {Lee, Jehee and Chai, Jinxiang and Reitsma, Paul S. A. and Hodgins, Jessica K. and Pollard, Nancy S.},
title = {Interactive control of avatars animated with human motion data},
year = {2002},
issue_date = {July 2002},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {3},
issn = {0730-0301},
url = {https://doi.org/10.1145/566654.566607},
doi = {10.1145/566654.566607},
abstract = {Real-time control of three-dimensional avatars is an important problem in the context of computer games and virtual environments. Avatar animation and control is difficult, however, because a large repertoire of avatar behaviors must be made available, and the user must be able to select from this set of behaviors, possibly with a low-dimensional input device. One appealing approach to obtaining a rich set of avatar behaviors is to collect an extended, unlabeled sequence of motion data appropriate to the application. In this paper, we show that such a motion database can be preprocessed for flexibility in behavior and efficient search and exploited for real-time avatar control. Flexibility is created by identifying plausible transitions between motion segments, and efficient search through the resulting graph structure is obtained through clustering. Three interface techniques are demonstrated for controlling avatar motion using this data structure: the user selects from a set of available choices, sketches a path through an environment, or acts out a desired motion in front of a video camera. We demonstrate the flexibility of the approach through four different applications and compare the avatar motion to directly recorded human motion.},
journal = {ACM Trans. Graph.},
month = {jul},
pages = {491â€“500},
numpages = {10},
keywords = {virtual environments, motion capture, interactive control, human motion, avatars}
}

@article{10.1145/3550454.3555454,
author = {Qin, Jia and Zheng, Youyi and Zhou, Kun},
title = {Motion In-Betweening via Two-Stage Transformers},
year = {2022},
issue_date = {December 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {41},
number = {6},
issn = {0730-0301},
url = {https://doi.org/10.1145/3550454.3555454},
doi = {10.1145/3550454.3555454},
abstract = {We present a deep learning-based framework to synthesize motion in-betweening in a two-stage manner. Given some context frames and a target frame, the system can generate plausible transitions with variable lengths in a non-autoregressive fashion. The framework consists of two Transformer Encoder-based networks operating in two stages: in the first stage a Context Transformer is designed to generate rough transitions based on the context and in the second stage a Detail Transformer is employed to refine motion details. Compared to existing Transformer-based methods which either use a complete Transformer Encoder-Decoder architecture or additional 1D convolutions to generate motion transitions, our framework achieves superior performance with less trainable parameters by only leveraging the Transformer Encoder and masked self-attention mechanism. To enhance the generalization of our transformer-based framework, we further introduce Keyframe Positional Encoding and Learned Relative Positional Encoding to make our method robust in synthesizing longer transitions exceeding the maximum transition length during training. Our framework is also artist-friendly by supporting full and partial pose constraints within the transition, giving artists fine control over the synthesized results. We benchmark our framework on the LAFAN1 dataset, and experiments show that our method outperforms the current state-of-the-art methods at a large margin (an average of 16\% for normal-length sequences and 55\% for excessive-length sequences). Our method trains faster than the RNN-based method and achieves a four-time speedup during inference. We implement our framework into a production-ready tool inside an animation authoring software and conduct a pilot study to validate the practical value of our method.},
journal = {ACM Trans. Graph.},
month = {nov},
articleno = {184},
numpages = {16},
keywords = {animation, deep learning, motion synthesis, transformer, transition generation}
}

@article{10.1145/3306346.3322938,
author = {Ciccone, Lo\"{\i}c and \"{O}ztireli, Cengiz and Sumner, Robert W.},
title = {Tangent-space optimization for interactive animation control},
year = {2019},
issue_date = {August 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {38},
number = {4},
issn = {0730-0301},
url = {https://doi.org/10.1145/3306346.3322938},
doi = {10.1145/3306346.3322938},
abstract = {Character animation tools are based on a keyframing metaphor where artists pose characters at selected keyframes and the software automatically interpolates the frames inbetween. Although the quality of the interpolation is critical for achieving a fluid and engaging animation, the tools available to adjust the result of the automatic inbetweening are rudimentary and typically require manual editing of spline parameters. As a result, artists spend a tremendous amount of time posing and setting more keyframes. In this pose-centric workflow, animators use combinations of forward and inverse kinematics. While forward kinematics leads to intuitive interpolations, it does not naturally support positional constraints such as fixed contact points. Inverse kinematics can be used to fix certain points in space at keyframes, but can lead to inferior interpolations, is slow to compute, and does not allow for positional contraints at non-keyframe frames. In this paper, we address these problems by formulating the control of interpolations with positional constraints over time as a space-time optimization problem in the tangent space of the animation curves driving the controls. Our method has the key properties that it (1) allows the manipulation of positions and orientations over time, extending inverse kinematics, (2) does not add new keyframes that might conflict with an artist's preferred keyframe style, and (3) works in the space of artist editable animation curves and hence integrates seamlessly with current pipelines. We demonstrate the utility of the technique in practice via various examples and use cases.},
journal = {ACM Trans. Graph.},
month = {jul},
articleno = {101},
numpages = {10},
keywords = {interpolation, inverse kinematics}
}

@inproceedings{inproceedings,
author = {Boulares, Mehrez and Jemni, Mohamed},
year = {2014},
month = {07},
pages = {474-481},
title = {SIGN MOTION : An Innovative Creation and Annotation Platform for Sign Language 3D-Content Corpora Building Relying on Low Cost Motion Sensors},
volume = {8548},
isbn = {978-3-319-08598-2},
doi = {10.1007/978-3-319-08599-9_71}
}

@inproceedings{filhol2018extending,
  title={Extending the AZee-Paula shortcuts to enable natural proform synthesis},
  author={Filhol, Michael and Mcdonald, John},
  booktitle={Workshop on the Representation and Processing of Sign Languages},
  year={2018}
}

