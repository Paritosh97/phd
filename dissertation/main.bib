@inproceedings{10.1145/218380.218422,
author = {Witkin, Andrew and Popovic, Zoran},
title = {Motion warping},
year = {1995},
isbn = {0897917014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/218380.218422},
doi = {10.1145/218380.218422},
booktitle = {Proceedings of the 22nd Annual Conference on Computer Graphics and Interactive Techniques},
pages = {105–108},
numpages = {4},
series = {SIGGRAPH '95}
}

@article{aristidou2011fabrik,
  title={FABRIK: A fast, iterative solver for the Inverse Kinematics problem},
  author={Aristidou, Andreas and Lasenby, Joan},
  journal={Graphical Models},
  volume={73},
  number={5},
  pages={243--260},
  year={2011},
  publisher={Elsevier}
}

@article{stokoe1980sign,
  title={Sign language structure},
  author={Stokoe, William C},
  journal={Annual review of anthropology},
  pages={365--390},
  year={1980},
  publisher={JSTOR}
}

@book{sutton1973sutton,
  title={Sutton movement shorthand},
  author={Sutton, Valerie},
  year={1973},
  publisher={Movement Shorthand Society}
}

@inproceedings{elliott2010towards,
  title={Towards the integration of synthetic SL animation with avatars into corpus annotation tools},
  author={Elliott, Ralph and Bueno, Javier and Kennaway, Richard and Glauert, John},
  booktitle={sign-lang@ LREC 2010},
  pages={84--87},
  year={2010},
  organization={Citeseer}
}

@article{mocialov2022unsupervised,
  title={Unsupervised Sign Language Phoneme Clustering using HamNoSys Notation},
  author={Mocialov, Boris and Turner, Graham and Hastie, Helen},
  journal={arXiv preprint arXiv:2205.10560},
  year={2022}
}

@phdthesis{huenerfauth2006generating,
  title={Generating American Sign Language classifier predicates for English-to-ASL machine translation},
  author={Huenerfauth, Matt},
  year={2006},
  school={Citeseer}
}

@article{elliott2004overview,
  title={An overview of the SiGML notation and SiGMLSigning software system},
  author={Elliott, Ralph and Glauert, John and Jennings, Vince and Kennaway, Richard},
  journal={sign-lang@ LREC 2004},
  pages={98--104},
  year={2004},
  publisher={European Language Resources Association (ELRA)}
}

@inproceedings{heloir2009embr,
  title={Embr--a realtime animation engine for interactive embodied agents},
  author={Heloir, Alexis and Kipp, Michael},
  booktitle={International Workshop on Intelligent Virtual Agents},
  pages={393--404},
  year={2009},
  organization={Springer}
}

@article{heloir2010real,
  title={Real-time animation of interactive agents: Specification and realization},
  author={Heloir, Alexis and Kipp, Michael},
  journal={Applied Artificial Intelligence},
  volume={24},
  number={6},
  pages={510--529},
  year={2010},
  publisher={Taylor \& Francis}
}

@inproceedings{azee-paula,
author = {Filhol, Michael and Mcdonald, John and Wolfe, Rosalee},
year = {2017},
month = {05},
pages = {27-40},
title = {Synthesizing Sign Language by Connecting Linguistically Structured Descriptions to a Multi-track Animation System},
isbn = {978-3-319-58702-8},
doi = {10.1007/978-3-319-58703-5_3}
}

@article{mcdonald2016automated,
  title={An automated technique for real-time production of lifelike animations of American Sign Language},
  author={McDonald, John and Wolfe, Rosalee and Schnepp, Jerry and Hochgesang, Julie and Jamrozik, Diana Gorman and Stumbo, Marie and Berke, Larwan and Bialek, Melissa and Thomas, Farah},
  journal={Universal Access in the Information Society},
  volume={15},
  number={4},
  pages={551--566},
  year={2016},
  publisher={Springer}
}

@article{eskimez,
  title={Noise-Resilient Training Method for Face Landmark Generation From Speech},
  author={Sefik Emre Eskimez and Ross K. Maddox and Chenliang Xu and Zhiyao Duan},
  journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  year={2020},
  volume={28},
  pages={27-38}
}

@inproceedings{greenwood18,
  title={Joint Learning of Facial Expression and Head Pose from Speech},
  author={David Greenwood and Iain Matthews and Stephen D. Laycock},
  booktitle={INTERSPEECH},
  year={2018}
}

@article{controllable_facial_synth,
  title={Pose-Controllable Talking Face Generation by Implicitly Modularized Audio-Visual Representation},
  author={Hang Zhou and Yasheng Sun and Wayne Wu and Chen Change Loy and Xiaogang Wang and Ziwei Liu},
  journal={2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2021},
  pages={4174-4184}
}

@inproceedings{nunnari2018animating,
  title={Animating azee descriptions using off-the-shelf ik solvers},
  author={Nunnari, Fabrizio and Filhol, Michael and Heloir, Alexis},
  booktitle={Workshop on the Representation and Processing of Sign Languages},
  year={2018}
}

@inbook{inbook,
author = {Carvalho, Schubert and Bouhe, R and Thalmann, Daniel and Egges, A and Geraerts, R and Overmars, M},
year = {2009},
month = {01},
pages = {116-127},
title = {Motion Pattern Encapsulation for Data-Driven Constraint-Based Motion Editing}
}

@article{zhang2022motiondiffuse,
      title   =   {MotionDiffuse: Text-Driven Human Motion Generation with Diffusion Model}, 
      author  =   {Zhang, Mingyuan and
                   Cai, Zhongang and
                   Pan, Liang and
                   Hong, Fangzhou and
                   Guo, Xinying and
                   Yang, Lei and
                   Liu, Ziwei},
      year    =   {2022},
      journal =   {arXiv preprint arXiv:2208.15001},
}

@article{Walsh2024ADR,
  title={A Data-Driven Representation for Sign Language Production},
  author={Harry Walsh and Abolfazl Ravanshad and Mariam Rahmani and Richard Bowden},
  journal={2024 IEEE 18th International Conference on Automatic Face and Gesture Recognition (FG)},
  year={2024},
  pages={1-10},
  url={https://api.semanticscholar.org/CorpusID:269187833}
}

@inproceedings{jiang-etal-2023-machine,
    title = "Machine Translation between Spoken Languages and Signed Languages Represented in {S}ign{W}riting",
    author = {Jiang, Zifan  and
      Moryossef, Amit  and
      M{\"u}ller, Mathias  and
      Ebling, Sarah},
    editor = "Vlachos, Andreas  and
      Augenstein, Isabelle",
    booktitle = "Findings of the Association for Computational Linguistics: EACL 2023",
    month = may,
    year = "2023",
    address = "Dubrovnik, Croatia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-eacl.127",
    doi = "10.18653/v1/2023.findings-eacl.127",
    pages = "1706--1724",
    abstract = "This paper presents work on novel machine translation (MT) systems between spoken and signed languages, where signed languages are represented in SignWriting, a sign language writing system. Our work seeks to address the lack of out-of-the-box support for signed languages in current MT systems and is based on the SignBank dataset, which contains pairs of spoken language text and SignWriting content. We introduce novel methods to parse, factorize, decode, and evaluate SignWriting, leveraging ideas from neural factored MT. In a bilingual setup{---}translating from American Sign Language to (American) English{---}our method achieves over 30 BLEU, while in two multilingual setups{---}translating in both directions between spoken languages and signed languages{---}we achieve over 20 BLEU. We find that common MT techniques used to improve spoken language translation similarly affect the performance of sign language translation. These findings validate our use of an intermediate text representation for signed languages to include them in natural language processing research.",
}

@inproceedings{yu2023signavatars,
    title = {SignAvatars: A Large-scale 3D Sign Language Holistic Motion Dataset and Benchmark},
    author = {Yu, Zhengdi and Huang, Shaoli and Cheng, Yongkang and Birdal, Tolga},
    month     = {October},
    year      = {2023}
}

@article{walsh2024sign,
  title={Sign Stitching: A Novel Approach to Sign Language Production},
  author={Walsh, Harry and Saunders, Ben and Bowden, Richard},
  journal={arXiv preprint arXiv:2405.07663},
  year={2024}
}

@inproceedings{filhol2024software,
  title={A software editor for the AZVD graphical Sign Language representation system},
  author={Filhol, Michael and Von Ascheberg, Thomas},
  booktitle={Proceedings of the LREC-COLING 2024 11th Workshop on the Representation and Processing of Sign Languages: Evaluation of Sign Language Resources},
  pages={77--85},
  year={2024}
}

@InProceedings{Forte_2023_CVPR,
    author    = {Forte, Maria-Paola and Kulits, Peter and Huang, Chun-Hao P. and Choutas, Vasileios and Tzionas, Dimitrios and Kuchenbecker, Katherine J. and Black, Michael J.},
    title     = {Reconstructing Signing Avatars From Video Using Linguistic Priors},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2023},
    pages     = {12791-12801}
}

@article{johnson2021towards,
  title={Towards enhanced visual clarity of sign language avatars through recreation of fine facial detail},
  author={Johnson, Ronan},
  journal={Machine Translation},
  volume={35},
  number={3},
  pages={431--445},
  year={2021},
  publisher={Springer}
}

@inproceedings{johnson-2022-improved,
    title = "Improved Facial Realism through an Enhanced Representation of Anatomical Behavior in Sign Language Avatars",
    author = "Johnson, Ronan",
    editor = "Efthimiou, Eleni  and
      Fotinea, Stavroula-Evita  and
      Hanke, Thomas  and
      McDonald, John C.  and
      Shterionov, Dimitar  and
      Wolfe, Rosalee",
    booktitle = "Proceedings of the 7th International Workshop on Sign Language Translation and Avatar Technology: The Junction of the Visual and the Textual: Challenges and Perspectives",
    month = jun,
    year = "2022",
    address = "Marseille, France",
    publisher = "European Language Resources Association",
    url = "https://aclanthology.org/2022.sltat-1.8",
    pages = "53--58",
    abstract = "Facial movements and expressions are critical features of signed languages, yet are some of the most challenging to reproduce on signing avatars. Due to the relative lack of research efforts in this area, the facial capabilities of such avatars have yet to receive the approval of those in the Deaf community. This paper revisits the representations of the human face in signed avatars, specifically those based on parameterized muscle simulation such as FACS and the MPEG-4 file definition. An improved framework based on rotational pivots and pre-defined movements is capable of reproducing realistic, natural gestures and mouthings on sign language avatars. The new approach is more harmonious with the underlying construction of signed avatars, generates improved results, and allows for a more intuitive workflow for the artists and animators who interact with the system.",
}

@article{azevedo2024empowering,
  title={Empowering Sign Language Communication: Integrating Sentiment and Semantics for Facial Expression Synthesis},
  author={Azevedo, Rafael and Coutinho, Thiago and Ferreira, Jo{\~a}o and Gomes, Thiago and Nascimento, Erickson},
  journal={arXiv preprint arXiv:2408.15159},
  year={2024}
}

@inproceedings{danvevcek2022emoca,
  title={Emoca: Emotion driven monocular face capture and animation},
  author={Dan{\v{e}}{\v{c}}ek, Radek and Black, Michael J and Bolkart, Timo},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={20311--20322},
  year={2022}
}

@article{luo2022learning,
  title={Learning multi-dimensional edge feature-based au relation graph for facial action unit recognition},
  author={Luo, Cheng and Song, Siyang and Xie, Weicheng and Shen, Linlin and Gunes, Hatice},
  journal={arXiv preprint arXiv:2205.01782},
  year={2022}
}

@article{ekman1978facial,
  title={Facial action coding system},
  author={Ekman, Paul and Friesen, Wallace V},
  journal={Environmental Psychology \& Nonverbal Behavior},
  year={1978}
}

@inproceedings{challant2024extending,
  title={Extending AZee with Non-manual Gesture Rules for French Sign Language},
  author={Challant, Camille and Filhol, Michael},
  booktitle={2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)},
  pages={7007--7016},
  year={2024},
  organization={European Language Resources Association}
}

@article{gilbert2021facshuman,
  title={FACSHuman, a software program for creating experimental material by modeling 3D facial expressions},
  author={Gilbert, Micha{\"e}l and Demarchi, Samuel and Urdapilleta, Isabel},
  journal={Behavior Research Methods},
  volume={53},
  number={5},
  pages={2252--2272},
  year={2021},
  publisher={Springer}
}


@article{yousaidthat,
  author    = {Amir Jamaludin and
               Joon Son Chung and
               Andrew Zisserman},
  title     = {You Said That?: Synthesising Talking Faces from Audio},
  journal   = {Int. J. Comput. Vis.},
  volume    = {127},
  number    = {11-12},
  pages     = {1767--1779},
  year      = {2019},
  url       = {https://doi.org/10.1007/s11263-019-01150-y},
  doi       = {10.1007/s11263-019-01150-y},
  timestamp = {Fri, 13 Mar 2020 00:00:00 +0100},
  biburl    = {https://dblp.org/rec/journals/ijcv/JamaludinCZ19.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{talkingface,
  title={Talking Face Generation by Conditional Recurrent Adversarial Network},
  author={Yang Song and Jingwen Zhu and Dawei Li and Xiaolong Wang and Hairong Qi},
  booktitle={IJCAI},
  year={2019}
}

@article{lipmovements,
  title={Lip Movements Generation at a Glance},
  author={Lele Chen and Zhiheng Li and Ross K. Maddox and Zhiyao Duan and Chenliang Xu},
  journal={ArXiv},
  year={2018},
  volume={abs/1803.10404}
}

@inbook{lipsyncexpert,
author = {Prajwal, K R and Mukhopadhyay, Rudrabha and Namboodiri, Vinay P. and Jawahar, C.V.},
title = {A Lip Sync Expert Is All You Need for Speech to Lip Generation In the Wild},
year = {2020},
isbn = {9781450379885},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3394171.3413532},
abstract = {In this work, we investigate the problem of lip-syncing a talking face video of an arbitrary identity to match a target speech segment. Current works excel at producing accurate lip movements on a static image or videos of specific people seen during the training phase. However, they fail to accurately morph the lip movements of arbitrary identities in dynamic, unconstrained talking face videos, resulting in significant parts of the video being out-of-sync with the new audio. We identify key reasons pertaining to this and hence resolve them by learning from a powerful lip-sync discriminator. Next, we propose new, rigorous evaluation benchmarks and metrics to accurately measure lip synchronization in unconstrained videos. Extensive quantitative evaluations on our challenging benchmarks show that the lip-sync accuracy of the videos generated by our Wav2Lip model is almost as good as real synced videos. We provide a demo video clearly showing the substantial impact of our Wav2Lip model, and also publicly release the code, models, and evaluation benchmarks on our website.},
booktitle = {Proceedings of the 28th ACM International Conference on Multimedia},
pages = {484–492},
numpages = {9}
}


@inbook{imitating,
author = {Wu, Haozhe and Jia, Jia and Wang, Haoyu and Dou, Yishun and Duan, Chao and Deng, Qingshan},
title = {Imitating Arbitrary Talking Style for Realistic Audio-Driven Talking Face Synthesis},
year = {2021},
isbn = {9781450386517},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3474085.3475280},
abstract = {People talk with diversified styles. For one piece of speech, different talking styles exhibit significant differences in the facial and head pose movements. For example, the "excited" style usually talks with the mouth wide open, while the "solemn" style is more standardized and seldomly exhibits exaggerated motions. Due to such huge differences between different styles, it is necessary to incorporate the talking style into audio-driven talking face synthesis framework. In this paper, we propose to inject style into the talking face synthesis framework through imitating arbitrary talking style of the particular reference video. Specifically, we systematically investigate talking styles with our collected Ted-HD dataset and construct style codes as several statistics of 3D morphable model (3DMM) parameters. Afterwards, we devise a latent-style-fusion (LSF) model to synthesize stylized talking faces by imitating talking styles from the style codes. We emphasize the following novel characteristics of our framework: (1) It doesn't require any annotation of the style, the talking style is learned in an unsupervised manner from talking videos in the wild. (2) It can imitate arbitrary styles from arbitrary videos, and the style codes can also be interpolated to generate new styles. Extensive experiments demonstrate that the proposed framework has the ability to synthesize more natural and expressive talking styles compared with baseline methods.},
booktitle = {Proceedings of the 29th ACM International Conference on Multimedia},
pages = {1478–1486},
numpages = {9}
}

@article{cudeiro,
  title={Capture, Learning, and Synthesis of 3D Speaking Styles},
  author={Daniel Cudeiro and Timo Bolkart and Cassidy Laidlaw and Anurag Ranjan and Michael J. Black},
  journal={2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2019},
  pages={10093-10103}
}

% FLAME
@inproceedings{FLAME,
  added-at = {2018-11-06T00:00:00.000+0100},
  author = {Wu, Yao and Ester, Martin},
  biburl = {https://www.bibsonomy.org/bibtex/29112be2dff89053b24a898251a888f92/dblp},
  booktitle = {WSDM},
  editor = {Cheng, Xueqi and Li, Hang and Gabrilovich, Evgeniy and Tang, Jie},
  ee = {https://doi.org/10.1145/2684822.2685291},
  interhash = {674401e807a00a82497444214dd2d8af},
  intrahash = {9112be2dff89053b24a898251a888f92},
  isbn = {978-1-4503-3317-7},
  keywords = {dblp},
  pages = {199-208},
  publisher = {ACM},
  timestamp = {2019-05-22T11:54:34.000+0200},
  title = {FLAME: A Probabilistic Model Combining Aspect Based Opinion Mining and Collaborative Filtering.},
  year = 2015
}

@article{Yang:2020:MakeItTalk,
	Author    = {Yang Zhou and Xintong Han and Eli Shechtman and Jose Echevarria and Evangelos Kalogerakis and Dingzeyu Li},
	Title     = {MakeItTalk: Speaker-Aware Talking-Head Animation},
	Journal   = {ACM Transactions on Graphics},
	Volume    = {39},
	Number    = {6},
	Year      = {2020},
}


@inproceedings{eamm,
author = {Ji, Xinya and Zhou, Hang and Wang, Kaisiyuan and Wu, Qianyi and Wu, Wayne and Xu, Feng and Cao, Xun},
title = {EAMM: One-Shot Emotional Talking Face via Audio-Based Emotion-Aware Motion Model},
year = {2022},
isbn = {9781450393379},
url = {https://doi.org/10.1145/3528233.3530745},
doi = {10.1145/3528233.3530745},
booktitle = {ACM SIGGRAPH 2022 Conference Proceedings},
series = {SIGGRAPH '22}
}

@inproceedings{challant2022first,
  title={A first corpus of azee discourse expressions},
  author={Challant, Camille and Filhol, Michael},
  booktitle={Language Resources and Evaluation Conference},
  year={2022}
}

@article{witkin1988spacetime,
  title={Spacetime constraints},
  author={Witkin, Andrew and Kass, Michael},
  journal={ACM Siggraph Computer Graphics},
  volume={22},
  number={4},
  pages={159--168},
  year={1988},
  publisher={ACM New York, NY, USA}
}

@article{10.1145/566654.566607,
author = {Lee, Jehee and Chai, Jinxiang and Reitsma, Paul S. A. and Hodgins, Jessica K. and Pollard, Nancy S.},
title = {Interactive control of avatars animated with human motion data},
year = {2002},
issue_date = {July 2002},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {3},
issn = {0730-0301},
url = {https://doi.org/10.1145/566654.566607},
doi = {10.1145/566654.566607},
abstract = {Real-time control of three-dimensional avatars is an important problem in the context of computer games and virtual environments. Avatar animation and control is difficult, however, because a large repertoire of avatar behaviors must be made available, and the user must be able to select from this set of behaviors, possibly with a low-dimensional input device. One appealing approach to obtaining a rich set of avatar behaviors is to collect an extended, unlabeled sequence of motion data appropriate to the application. In this paper, we show that such a motion database can be preprocessed for flexibility in behavior and efficient search and exploited for real-time avatar control. Flexibility is created by identifying plausible transitions between motion segments, and efficient search through the resulting graph structure is obtained through clustering. Three interface techniques are demonstrated for controlling avatar motion using this data structure: the user selects from a set of available choices, sketches a path through an environment, or acts out a desired motion in front of a video camera. We demonstrate the flexibility of the approach through four different applications and compare the avatar motion to directly recorded human motion.},
journal = {ACM Trans. Graph.},
month = {jul},
pages = {491–500},
numpages = {10},
keywords = {virtual environments, motion capture, interactive control, human motion, avatars}
}

@article{10.1145/3550454.3555454,
author = {Qin, Jia and Zheng, Youyi and Zhou, Kun},
title = {Motion In-Betweening via Two-Stage Transformers},
year = {2022},
issue_date = {December 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {41},
number = {6},
issn = {0730-0301},
url = {https://doi.org/10.1145/3550454.3555454},
doi = {10.1145/3550454.3555454},
abstract = {We present a deep learning-based framework to synthesize motion in-betweening in a two-stage manner. Given some context frames and a target frame, the system can generate plausible transitions with variable lengths in a non-autoregressive fashion. The framework consists of two Transformer Encoder-based networks operating in two stages: in the first stage a Context Transformer is designed to generate rough transitions based on the context and in the second stage a Detail Transformer is employed to refine motion details. Compared to existing Transformer-based methods which either use a complete Transformer Encoder-Decoder architecture or additional 1D convolutions to generate motion transitions, our framework achieves superior performance with less trainable parameters by only leveraging the Transformer Encoder and masked self-attention mechanism. To enhance the generalization of our transformer-based framework, we further introduce Keyframe Positional Encoding and Learned Relative Positional Encoding to make our method robust in synthesizing longer transitions exceeding the maximum transition length during training. Our framework is also artist-friendly by supporting full and partial pose constraints within the transition, giving artists fine control over the synthesized results. We benchmark our framework on the LAFAN1 dataset, and experiments show that our method outperforms the current state-of-the-art methods at a large margin (an average of 16\% for normal-length sequences and 55\% for excessive-length sequences). Our method trains faster than the RNN-based method and achieves a four-time speedup during inference. We implement our framework into a production-ready tool inside an animation authoring software and conduct a pilot study to validate the practical value of our method.},
journal = {ACM Trans. Graph.},
month = {nov},
articleno = {184},
numpages = {16},
keywords = {animation, deep learning, motion synthesis, transformer, transition generation}
}

@article{10.1145/3306346.3322938,
author = {Ciccone, Lo\"{\i}c and \"{O}ztireli, Cengiz and Sumner, Robert W.},
title = {Tangent-space optimization for interactive animation control},
year = {2019},
issue_date = {August 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {38},
number = {4},
issn = {0730-0301},
url = {https://doi.org/10.1145/3306346.3322938},
doi = {10.1145/3306346.3322938},
abstract = {Character animation tools are based on a keyframing metaphor where artists pose characters at selected keyframes and the software automatically interpolates the frames inbetween. Although the quality of the interpolation is critical for achieving a fluid and engaging animation, the tools available to adjust the result of the automatic inbetweening are rudimentary and typically require manual editing of spline parameters. As a result, artists spend a tremendous amount of time posing and setting more keyframes. In this pose-centric workflow, animators use combinations of forward and inverse kinematics. While forward kinematics leads to intuitive interpolations, it does not naturally support positional constraints such as fixed contact points. Inverse kinematics can be used to fix certain points in space at keyframes, but can lead to inferior interpolations, is slow to compute, and does not allow for positional contraints at non-keyframe frames. In this paper, we address these problems by formulating the control of interpolations with positional constraints over time as a space-time optimization problem in the tangent space of the animation curves driving the controls. Our method has the key properties that it (1) allows the manipulation of positions and orientations over time, extending inverse kinematics, (2) does not add new keyframes that might conflict with an artist's preferred keyframe style, and (3) works in the space of artist editable animation curves and hence integrates seamlessly with current pipelines. We demonstrate the utility of the technique in practice via various examples and use cases.},
journal = {ACM Trans. Graph.},
month = {jul},
articleno = {101},
numpages = {10},
keywords = {interpolation, inverse kinematics}
}

@inproceedings{inproceedings,
author = {Boulares, Mehrez and Jemni, Mohamed},
year = {2014},
month = {07},
pages = {474-481},
title = {SIGN MOTION : An Innovative Creation and Annotation Platform for Sign Language 3D-Content Corpora Building Relying on Low Cost Motion Sensors},
volume = {8548},
isbn = {978-3-319-08598-2},
doi = {10.1007/978-3-319-08599-9_71}
}

@inproceedings{filhol2018extending,
  title={Extending the AZee-Paula shortcuts to enable natural proform synthesis},
  author={Filhol, Michael and Mcdonald, John},
  booktitle={Workshop on the Representation and Processing of Sign Languages},
  year={2018}
}

@inproceedings{mcdonald2019fine,
  title={Fine tuning dynamics in contextualized proform constructs from linguistic descriptions},
  author={Mcdonald, John and Filhol, Michael},
  booktitle={International Workshop on Sign Language Translation and Avatar Technology},
  year={2019}
}

@article{mcdonald2021natural,
  title={Natural synthesis of productive forms from structured descriptions of sign language},
  author={McDonald, John and Filhol, Michael},
  journal={Machine Translation},
  volume={35},
  number={3},
  pages={363--386},
  year={2021},
  publisher={Springer}
}

@inproceedings{filhol2020synthesis,
  title={The synthesis of complex shape deployments in sign language},
  author={Filhol, Michael and McDonald, John},
  booktitle={Proceedings of the 9th workshop on the Representation and Processing of Sign Languages},
  year={2020}
}

@inproceedings{sharma:hal-03721720,
  TITLE = {{Multi-Track Bottom-Up Synthesis from Non-Flattened AZee Scores}},
  AUTHOR = {Sharma, Paritosh and Filhol, Michael},
  URL = {https://hal.archives-ouvertes.fr/hal-03721720},
  BOOKTITLE = {{7th Workshop on Sign Language Translation and Avatar Technology: The Junction of the Visual \& the Textual Challenges and Perspectives (SLTAT 7)}},
  ADDRESS = {Marseille, France},
  YEAR = {2022},
  MONTH = Jun,
  KEYWORDS = {sign language ; avatar ; AZee},
  PDF = {https://hal.archives-ouvertes.fr/hal-03721720/file/70018_Paper.pdf},
  HAL_ID = {hal-03721720},
  HAL_VERSION = {v1},
}

@inproceedings{sharma:hal-04143663,
  TITLE = {{A Layered Approach to Constrain Signing Avatars}},
  AUTHOR = {Sharma, Paritosh},
  URL = {https://hal.science/hal-04143663},
  BOOKTITLE = {{VISIGRAPP\_DC 2023}},
  ADDRESS = {Lisbon, Portugal},
  ORGANIZATION = {{Scitevents}},
  YEAR = {2023},
  MONTH = Feb,
  KEYWORDS = {Animation ; Sign language ; AZee},
  PDF = {https://hal.science/hal-04143663/file/GRAPP_DC.pdf},
  HAL_ID = {hal-04143663},
  HAL_VERSION = {v1},
}

@INPROCEEDINGS{10193413,
  author={Sharma, Paritosh and Filhol, Michael},
  booktitle={2023 IEEE International Conference on Acoustics, Speech, and Signal Processing Workshops (ICASSPW)}, 
  title={Extending Morphs in AZee Using Pose Space Deformations}, 
  year={2023},
  volume={},
  number={},
  pages={1-5},
  doi={10.1109/ICASSPW59220.2023.10193413}}

@inproceedings{10.1145/3606037.3606837,
author = {Sharma, Paritosh and Filhol, Michael},
title = {Intermediate Block Generation for Multi-Track Sign Language Synthesis},
year = {2023},
isbn = {9798400702686},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3606037.3606837},
doi = {10.1145/3606037.3606837},
abstract = {Generating realistic Sign Language using signing avatars is a challenging task that typically involves synthesis using either procedural or pre-animated techniques like motion capture or artistic editing of signs. However, combining these two approaches is difficult. In this work, we propose a novel method for generating intermediate poses in a multi-track representation of a sign language discourse. The proposed method uses procedural generation with artistic techniques to prioritize certain aspects of the generated poses while sacrificing others to improve the overall consistency of the representation. The system is implemented as an add-on in Blender, an open-source 3D toolkit.},
booktitle = {Proceedings of the ACM SIGGRAPH/Eurographics Symposium on Computer Animation},
articleno = {3},
numpages = {2},
keywords = {sign language, motion retargeting, animation},
location = {Los Angeles, CA, USA},
series = {SCA '23}
}

@inproceedings{Sharma2024FacialEF,
  title={Facial Expressions for Sign Language Synthesis using FACSHuman and AZee},
  author={Paritosh Sharma and Camille Challant and Michael Filhol},
  booktitle={SIGNLANG},
  year={2024},
  url={https://api.semanticscholar.org/CorpusID:269950872}
}

@inproceedings{10.1145/3658852.3659080,
author = {Sharma, Paritosh and Filhol, Michael},
title = {Sign Language Synthesis using Pose Priors},
year = {2024},
isbn = {9798400709944},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3658852.3659080},
doi = {10.1145/3658852.3659080},
abstract = {The challenge of simulating realistic Sign Language using avatars lies in achieving accurate human-like postures for effective communication. Unlike artistic or motion capture techniques, linguist-driven procedural generation methods are widely employed, relying on skeletal representations to synthesize a broad range of signs. However, determining appropriate joint limits for these avatars is intricate due to inter-joint and intra-joint dependencies, as well as variations in biomechanical properties. In this context, our work addresses this problem by introducing a pose corrector, enhancing an established Sign Language synthesis technique. Focused on rectifying extreme joint rotations, our approach incorporates a pre-trained poser based on existing work, integrated with a 21-joint character model. The correction process involves applying linguist-defined constraints using AZee language and subsequent pose corrections, showcasing promising advancements in obtaining more natural sign gestures.},
booktitle = {Proceedings of the 9th International Conference on Movement and Computing},
articleno = {15},
numpages = {4},
keywords = {Avatar, Motion Synthesis, Procedural Animation, Sign Language},
location = {Utrecht, Netherlands},
series = {MOCO '24}
}

@INPROCEEDINGS{4648032,  author={Smits, Ruben and De Laet, Tinne and Claes, Kasper and Bruyninckx, Herman and De Schutter, Joris},  booktitle={2008 IEEE International Conference on Multisensor Fusion and Integration for Intelligent Systems},   title={iTASC: a tool for multi-sensor integration in robot manipulation},   year={2008},  volume={},  number={},  pages={426-433},  doi={10.1109/MFI.2008.4648032}}

@article{kenwright2012inverse,
  title={Inverse kinematics--cyclic coordinate descent (ccd)},
  author={Kenwright, Ben},
  journal={Journal of Graphics Tools},
  volume={16},
  number={4},
  pages={177--217},
  year={2012},
  publisher={Taylor \& Francis}
}

@article{10.1145/3072959.3073663,
author = {Holden, Daniel and Komura, Taku and Saito, Jun},
title = {Phase-functioned neural networks for character control},
year = {2017},
issue_date = {August 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {36},
number = {4},
issn = {0730-0301},
url = {https://doi.org/10.1145/3072959.3073663},
doi = {10.1145/3072959.3073663},
abstract = {We present a real-time character control mechanism using a novel neural network architecture called a Phase-Functioned Neural Network. In this network structure, the weights are computed via a cyclic function which uses the phase as an input. Along with the phase, our system takes as input user controls, the previous state of the character, the geometry of the scene, and automatically produces high quality motions that achieve the desired user control. The entire network is trained in an end-to-end fashion on a large dataset composed of locomotion such as walking, running, jumping, and climbing movements fitted into virtual environments. Our system can therefore automatically produce motions where the character adapts to different geometric environments such as walking and running over rough terrain, climbing over large rocks, jumping over obstacles, and crouching under low ceilings. Our network architecture produces higher quality results than time-series autoregressive models such as LSTMs as it deals explicitly with the latent variable of motion relating to the phase. Once trained, our system is also extremely fast and compact, requiring only milliseconds of execution time and a few megabytes of memory, even when trained on gigabytes of motion data. Our work is most appropriate for controlling characters in interactive scenes such as computer games and virtual reality systems.},
journal = {ACM Trans. Graph.},
month = {jul},
articleno = {42},
numpages = {13},
keywords = {character animation, character control, deep learning, human motion, locomotion, neural networks}
}

@incollection{grochow2004style,
  title={Style-based inverse kinematics},
  author={Grochow, Keith and Martin, Steven L and Hertzmann, Aaron and Popovi{\'c}, Zoran},
  booktitle={ACM SIGGRAPH 2004 Papers},
  pages={522--531},
  year={2004}
}

@article{kingma2013auto,
  title={Auto-encoding variational bayes},
  author={Kingma, Diederik P},
  journal={arXiv preprint arXiv:1312.6114},
  year={2013}
}

@inproceedings{pavlakos2019expressive,
  title={Expressive body capture: 3d hands, face, and body from a single image},
  author={Pavlakos, Georgios and Choutas, Vasileios and Ghorbani, Nima and Bolkart, Timo and Osman, Ahmed AA and Tzionas, Dimitrios and Black, Michael J},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={10975--10985},
  year={2019}
}

@inproceedings{bertin2022rosetta,
  title={Rosetta-lsf: an aligned corpus of french sign language and french for text-to-sign translation},
  author={Bertin-Lem{\'e}e, {\'E}lise and Braffort, Annelies and Challant, Camille and Danet, Claire and Dauriac, Boris and Filhol, Michael and Martinod, Emmanuella and Segouat, J{\'e}r{\'e}mie},
  booktitle={13th Conference on Language Resources and Evaluation (LREC 2022)},
  year={2022}
}

@inproceedings{tiwari2022pose,
  title={Pose-ndf: Modeling human pose manifolds with neural distance fields},
  author={Tiwari, Garvita and Anti{\'c}, Dimitrije and Lenssen, Jan Eric and Sarafianos, Nikolaos and Tung, Tony and Pons-Moll, Gerard},
  booktitle={European Conference on Computer Vision},
  pages={572--589},
  year={2022},
  organization={Springer}
}

@article{lu2023dposer,
  title={DPoser: Diffusion Model as Robust 3D Human Pose Prior},
  author={Lu, Junzhe and Lin, Jing and Dou, Hongkun and Zhang, Yulun and Deng, Yue and Wang, Haoqian},
  journal={arXiv preprint arXiv:2312.05541},
  year={2023}
}

@inproceedings{petrovich24stmc,
    title     = {Multi-Track Timeline Control for Text-Driven 3D Human Motion Generation},
    author    = {Petrovich, Mathis and Litany, Or and Iqbal, Umar and Black, Michael J. and Varol, G{\"u}l and Peng, Xue Bin and Rempe, Davis},
    booktitle = {CVPR Workshop on Human Motion Generation},
    year      = {2024}
}

@inproceedings{filhol2017synthesizing,
  title={Synthesizing sign language by connecting linguistically structured descriptions to a multi-track animation system},
  author={Filhol, Michael and McDonald, John and Wolfe, Rosalee},
  booktitle={Universal Access in Human--Computer Interaction. Designing Novel Interactions: 11th International Conference, UAHCI 2017, Held as Part of HCI International 2017, Vancouver, BC, Canada, July 9--14, 2017, Proceedings, Part II 11},
  pages={27--40},
  year={2017},
  organization={Springer}
}

@misc{jiang2024signclipconnectingtextsign,
      title={SignCLIP: Connecting Text and Sign Language by Contrastive Learning}, 
      author={Zifan Jiang and Gerard Sant and Amit Moryossef and Mathias Müller and Rico Sennrich and Sarah Ebling},
      year={2024},
      eprint={2407.01264},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2407.01264}, 
}

@misc{moryossef2024signmtrealtimemultilingualsign,
      title={sign.mt: Real-Time Multilingual Sign Language Translation Application}, 
      author={Amit Moryossef},
      year={2024},
      eprint={2310.05064},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2310.05064}, 
}


@Book{moody97,
  title  = {La langue des signes, dictionnaire bilingue \'el\'ementaire},
  year   = {1997},
  author = {Moody, Bill},
  editor = {IVT Paris},
}

@misc{saunders2020everybodysignnowtranslating,
      title={Everybody Sign Now: Translating Spoken Language to Photo Realistic Sign Language Video}, 
      author={Ben Saunders and Necati Cihan Camgoz and Richard Bowden},
      year={2020},
      eprint={2011.09846},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2011.09846}, 
}